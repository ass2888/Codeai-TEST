import express from 'express';
import cors from 'cors';
import bodyParser from 'body-parser';
import path from 'path';
import { fileURLToPath } from 'url';
import { dirname } from 'path';
import { GoogleGenerativeAI } from '@google/generative-ai';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);
const DEFAULT_REASONING_MODEL = "trinity-large";
const OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions";
const providerHealth = {
  openrouter: { blockedUntil: 0 },
  google: { blockedUntil: 0 }
};

const PROVIDER_COOLDOWN = 60_000; // 60 Ø«Ø§Ù†ÙŠØ©


// === Codeai code-R 1.0 | Fallback Map ===
// === Codeai code-R 1.0 | FINAL FALLBACK MAP ===
// === Codeai code-R | FINAL FALLBACK MAP (OFFICIAL) ===
const STATION_FALLBACKS = {

  A: [ // âš¡ Fast / General / Chat
    "llama-3.1-instant",        // Groq â€“ Ø£Ø³Ø±Ø¹ chat
    "gemini-3-flash",
    "gpt-oss"                  // OpenRouter fallback
  ],

  B: [ // ğŸ§  Deep Reasoning (THINKING)
    "gpt-oss-120b",            // â­ Groq â€“ ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚
    "trinity-large",
    "chimera-r1",
    "gemini-2.5-pro"           // Ø¢Ø®Ø± Ø­Ù„
  ],

  C: [ // ğŸ’» Coding / Execution
    "qwen-coder",
    "llama-3.3-70b",           // Groq â€“ Ù…Ù…ØªØ§Ø² Ù„Ù„ÙƒÙˆØ¯
    "gpt-oss-120b",            // ØªØ­Ù„ÙŠÙ„ + ÙƒÙˆØ¯
    "chimera-r1",
    "gemini-3-flash"           // emergency
  ]
};


// --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø­Ø¯ÙˆØ¯ (Ù…Ø«Ø§Ù„ Ù„Ù€ Gemini Flash) ---
const LIMITS = {
    GEMINI: {
        RPM: 3,
        TPM: 230000,
        RPD: 17,
    },
    GEMMA: {
        RPM: 27,      // Gemma Ù„Ù‡ Ø­Ø¯ÙˆØ¯ Ø£Ø¹Ù„Ù‰
        TPM: 12000,
        RPD: 12000,
    },
    OPENROUTER: { // Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ©
        RPM: 20,     
        TPM: 40000,
        RPD: 500,
    },
    KIMI: { // ğŸ‘ˆ Ø¥Ø¶Ø§ÙØ© Ø¬Ø¯ÙŠØ¯Ø©
        RPM: 17,       // Ø¹Ø¯Ù„ Ø­Ø³Ø¨ Ø­Ø¯ÙˆØ¯ Ø­Ø³Ø§Ø¨Ùƒ ÙÙŠ Moonshot
        TPM: 470000,
        RPD: 200,
    },
    GROQ: {
        RPM: 30,
        TPM: 1_000_000, // Ø¹Ù…Ù„ÙŠØ§Ù‹ ØºÙŠØ± Ù…Ø­Ø¯ÙˆØ¯
        RPD: 1000
    }
};

// Ø£Ø¶Ù ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø¹Ø¯ ØªØ¹Ø±ÙŠÙ LIMITS
// Ø¨Ø¹Ø¯ ØªØ¹Ø±ÙŠÙ LIMITS Ø£Ø¶Ù:
const MODEL_CONFIGS = {
    'gemini-3-flash': {
        provider: 'google',
        modelName: 'gemini-3-flash-preview',
        displayName: 'Gemini 3 Flash',
        maxTokens: 100000,
        temperature: 0.7,
        supportsStreaming: true,
        features: ['fast', 'latest']
    },
    'gemini-2.5-pro': {
        provider: 'google',
        modelName: 'gemini-2.5-pro',
        displayName: 'Gemini 2.5 Pro',
        maxTokens: 1000000,
        temperature: 0.7,
        supportsStreaming: false,
        features: ['long-context', 'reasoning', 'advanced']
    },
    'gemini-2.5': {
        provider: 'google',
        modelName: 'gemini-2.5-flash',
        displayName: 'Gemini 2.5',
        maxTokens: 100000,
        temperature: 0.7,
        supportsStreaming: true,
        features: ['fast', 'efficient', 'balanced']
    },
    'gemma': {
     provider: "google",
     modelName: "gemma-3-27b-it",
     maxTokens: 8192,
     temperature: 0.3,
     supportsStreaming: false,
     displayName: "Gemma 3 12B"
    },
    'deepseek-coder': {
        provider: 'deepseek',
        modelName: 'deepseek-coder',
        displayName: 'DeepSeek Coder',
        maxTokens: 16000,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://api.deepseek.com/v1/chat/completions',
        features: ['coding', 'open-source']
    },
    'deepseek-chat': {
        provider: 'deepseek',
        modelName: 'deepseek-chat',
        displayName: 'DeepSeek Chat',
        maxTokens: 16000,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://api.deepseek.com/v1/chat/completions',
        features: ['general', 'open-source']
    },
      'qwen-coder': {
        provider: 'openrouter',
        modelName: 'qwen/qwen3-coder:free', // ØªØµØ­ÙŠØ­ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø´Ø§Ø¦Ø¹ØŒ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… qwen/qwen3-coder:free Ø¥Ø°Ø§ ØªÙˆÙØ±
        displayName: 'Qwen 3 Coder 480B',
        maxTokens: 32000,
        temperature: 0.6,
        supportsStreaming: true,
        apiUrl: 'https://openrouter.ai/api/v1/chat/completions',
        features: ['coding']
    },
    'chimera-r1': {
        provider: 'openrouter',
        modelName: 'tngtech/deepseek-r1t2-chimera:free',
        displayName: 'DeepSeek (Chimera R1T2)',
        maxTokens: 32000,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://openrouter.ai/api/v1/chat/completions',
        features: ['reasoning',]
    },
    'hermes-3': {
        provider: 'openrouter',
        modelName: 'nousresearch/hermes-3-llama-3.1-405b:free',
        displayName: 'Hermes 3 405B',
        maxTokens: 4096,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://openrouter.ai/api/v1/chat/completions',
        features: ['large', 'free']
    },
    'gpt-oss': {
        provider: 'openrouter',
        modelName: 'openai/gpt-oss-20b:free',
        displayName: 'GPT-OSS 20B',
        maxTokens: 4096,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://openrouter.ai/api/v1/chat/completions',
        features: ['free']
    },
    'solar-pro': {
        provider: 'openrouter',
        modelName: 'upstage/solar-pro-3:free',
        displayName: 'Solar Pro 3',
        maxTokens: 4096,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://openrouter.ai/api/v1/chat/completions',
        features: ['efficient', 'free']
    },
    'trinity-large': {
        provider: 'openrouter',
        modelName: 'arcee-ai/trinity-large-preview:free',
        displayName: 'Trinity Large 400B',
        maxTokens: 4096,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://openrouter.ai/api/v1/chat/completions',
        features: ['advanced', 'free']
    },
    'kimi-k2': {
        provider: "kimi",
        modelName: "moonshot-v1-8k",
        maxTokens: 8000,
        temperature: 0.3,
        supportsStreaming: false,
        displayName: "Kimi K2"
    },
    'llama-3.1-instant': {
     provider: 'groq',
     modelName: 'llama-3.1-8b-instant',
     displayName: 'LLaMA 3.1 8B Instant',
     maxTokens: 8000,
     temperature: 0.7,
     supportsStreaming: true,
     features: ['fast', 'chat']
    },
    'llama-3.3-70b': {
     provider: 'groq',
     modelName: 'llama-3.3-70b-versatile',
     displayName: 'LLaMA 3.3 70B',
     maxTokens: 12000,
     temperature: 0.6,
     supportsStreaming: true,
     features: ['coding', 'reasoning']
    },
    'groq-compound': {
     provider: 'groq',
     modelName: 'groq/compound',
     displayName: 'Groq Compound',
     maxTokens: 16000,
     temperature: 0.3,
     supportsStreaming: false,
     features: ['reasoning', 'no-tpm-limit']
    },
    'gpt-oss-120b': {
     provider: 'groq',
     modelName: 'openai/gpt-oss-120b',
     displayName: 'GPT-OSS 120B',
     maxTokens: 8000,
     temperature: 0.3,
     supportsStreaming: false,
     features: ['analysis', 'reasoning', 'large']
    },
};

const D1 = process.env.D1; // D = deepseek
const G3 = process.env.G3; 
const G2 = process.env.G2;
const G1 = process.env.G1; // G = Gemini
const O1 = process.env.O1; // O = Openrouter
const O2 = process.env.O2;
const K1 = process.env.K1;
const R1 = process.env.R1;

// ÙƒØ§Ø¦Ù† Ù„ØªØªØ¨Ø¹ Ø§Ù„Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ù„ÙƒÙ„ Ù…ÙØªØ§Ø­
// ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ù…Ù„Ø© Ù„Ù€ usageStats
let usageStats = {};

// ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙØ§ØªÙŠØ­
// ØºÙŠÙ‘Ø± Ø§Ù„Ø³Ø·Ø± Ù„ÙŠØµØ¨Ø­:
['G1', 'G2', 'G3', 'D1', 'O1', 'O2', 'K1', 'R1'].forEach(keyId => {
    usageStats[keyId] = {
        gemini: { rpm: 0, tpm: 0, rpd: 0, lastMinute: Date.now(), lastDay: Date.now() },
        gemma: { rpm: 0, tpm: 0, rpd: 0, lastMinute: Date.now(), lastDay: Date.now() },
        deepseek: { rpm: 0, tpm: 0, rpd: 0, lastMinute: Date.now(), lastDay: Date.now() },
        openrouter: { rpm: 0, tpm: 0, rpd: 0, lastMinute: Date.now(), lastDay: Date.now() }, // â¬… Ø¬Ø¯ÙŠØ¯
        kimi: { rpm: 0, tpm: 0, rpd: 0, lastMinute: Date.now(), lastDay: Date.now() },
        groq: { rpm: 0, tpm: 0, rpd: 0, lastMinute: Date.now(), lastDay: Date.now() }
    };
});

console.log("âœ… Initialized usage stats for all keys");

/**
 * Ø¯Ø§Ù„Ø© Ù„ØªØµÙÙŠØ± Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ù†Ø¯ Ù…Ø±ÙˆØ± Ø¯Ù‚ÙŠÙ‚Ø© Ø£Ùˆ ÙŠÙˆÙ…
 */
function refreshStats(keyId, modelType) {
    const now = Date.now();
    const stats = usageStats[keyId][modelType];
    
    // ØªØµÙÙŠØ± Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
    if (now - stats.lastMinute > 60000) {
        stats.rpm = 0;
        stats.tpm = 0;
        stats.lastMinute = now;
    }
    // ØªØµÙÙŠØ± Ø§Ù„ÙŠÙˆÙ…
    if (now - stats.lastDay > 86400000) {
        stats.rpd = 0;
        stats.lastDay = now;
    }
}


function isProviderAvailable(provider) {
  return Date.now() > (providerHealth[provider]?.blockedUntil || 0);
}

function markProviderRateLimited(provider) {
  providerHealth[provider] = {
    blockedUntil: Date.now() + PROVIDER_COOLDOWN
  };
}

function selectModelForStation(stationKey) {
  const candidates = STATION_FALLBACKS[stationKey];
  if (!candidates) return null;

  for (const modelId of candidates) {
    const config = MODEL_CONFIGS[modelId];
    if (!config) continue;

    if (isProviderAvailable(config.provider)) {
      return modelId;
    }
  }

  return null; // ÙƒÙ„ Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ† Ù…Ø­Ø¬ÙˆØ¨ÙŠÙ†
}

function getNextFallbackModel(stationKey, currentModel) {
  const list = STATION_FALLBACKS[stationKey];
  if (!list) return null;

  const idx = list.indexOf(currentModel);
  return list[idx + 1] || null;
}

function getNextAvailableModel(startModel) {
  let found = false;

  for (const station of FALLBACK_STATIONS) {
    for (const modelId of station) {
      if (modelId === startModel) {
        found = true;
      }
      if (!found) continue;

      const config = MODEL_CONFIGS[modelId];
      if (!config) continue;

      if (!isProviderAvailable(config.provider)) {
        continue;
      }

      const key = getSafeKeyForModel(modelId);
      if (key) {
        return modelId;
      }
    }
  }

  return null;
}

/**
 * Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Ø¢Ù…Ù† Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ÙŠÙ†
 */
/**
 * Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Ø¢Ù…Ù† Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ÙŠÙ† (Ù…Ø¹Ø¯Ù„ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªØ¯Ø§Ø®Ù„)
 */
function getSafeKey(modelType = 'gemini') {
    const keys = ['G1', 'G2', 'G3'];
    const limits = LIMITS[modelType.toUpperCase()];
    
    console.log(`ğŸ” Looking for ${modelType} key. Available keys: ${keys}`);
    
    for (let keyId of keys) {
        const keyToken = process.env[keyId];
        if (!keyToken) {
            console.log(`   ${keyId}: No token available`);
            continue;
        }

        refreshStats(keyId, modelType);
        const stats = usageStats[keyId][modelType];

        // ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù‚ÙŠÙ… Ù„ÙŠØ³Øª NaN
        const currentRpm = isNaN(stats.rpm) ? 0 : stats.rpm;
        const currentTpm = isNaN(stats.tpm) ? 0 : stats.tpm;
        const currentRpd = isNaN(stats.rpd) ? 0 : stats.rpd;

        const isRpmSafe = currentRpm < (limits.RPM - 1);
        const isTpmSafe = currentTpm < (limits.TPM * 0.9);
        const isRpdSafe = currentRpd < limits.RPD;
        
        console.log(`   ${keyId}: RPM=${currentRpm}/${limits.RPM}, TPM=${currentTpm}/${limits.TPM}, RPD=${currentRpd}/${limits.RPD}`);
        
        if (isRpmSafe && isTpmSafe && isRpdSafe) {
            console.log(`âœ… Selected ${modelType} Key: ${keyId}`);
            return { 
                id: keyId, 
                token: keyToken,
                modelType: modelType
            };
        } else {
            console.log(`   ${keyId}: Limits exceeded`);
        }
    }
    
    console.log(`âŒ No available keys for ${modelType}`);
    
}

function getSafeKeyForModel(requestedModel) {
    const modelConfig = MODEL_CONFIGS[requestedModel];
    if (!modelConfig) {
        console.log(`âŒ Model config not found: ${requestedModel}`);
        return getSafeKey();
    }
    
    // ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ ÙˆØ§Ù„Ù…ÙØ§ØªÙŠØ­ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø²ÙˆØ¯
    let limits, keys, modelType;
    
    if (modelConfig.provider === 'deepseek') {
        limits = { RPM: 60, TPM: 1000000, RPD: 1000 };
        keys = ['D1'];
        modelType = 'deepseek';
    } else if (modelConfig.provider === 'openrouter') { // â¬… Ø­Ø§Ù„Ø© OpenRouter
        limits = LIMITS.OPENROUTER;
        keys = ['O1', 'O2'];
        modelType = 'openrouter';
    } else if (modelConfig.provider === 'kimi') { // ğŸ‘ˆ Ø¥Ø¶Ø§ÙØ© Ø´Ø±Ø· Ø¬Ø¯ÙŠØ¯
        limits = LIMITS.KIMI;
        keys = ['K1'];
        modelType = 'kimi'; // ØªØ£ÙƒØ¯ Ø£Ù†Ùƒ Ø£Ø¶ÙØª Ù‡Ø°Ø§ Ø§Ù„Ù†ÙˆØ¹ ÙÙŠ usageStats ÙÙŠ Ø§Ù„Ø®Ø·ÙˆØ© 1
    } else if (modelConfig.provider === 'groq') {
        limits = LIMITS.GROQ;
        keys = ['R1'];
        modelType = 'groq';
    
    } else { // Google
        limits = { RPM: 3, TPM: 230000, RPD: 17 };
        keys = ['G1', 'G2', 'G3'];
        modelType = 'gemini';
    }
    
    for (let keyId of keys) {
        const keyToken = process.env[keyId];
        if (!keyToken) continue;
        
        refreshStats(keyId, modelType);
        const stats = usageStats[keyId][modelType];
        
        const isRpmSafe = stats.rpm < (limits.RPM - 1);
        const isTpmSafe = stats.tpm < (limits.TPM * 0.9);
        const isRpdSafe = stats.rpd < limits.RPD;
        
        if (isRpmSafe && isTpmSafe && isRpdSafe) {
            return { 
                id: keyId, 
                token: keyToken,
                provider: modelConfig.provider,
                modelConfig: modelConfig
            };
        }
    }

}

async function sendResponseUnified({
  model,
  prompt,
  supportsStreaming,
  provider,
  keyInfo,
  onChunk,
  onComplete,
  maxRetries = 2 // Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù…Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
}) {
  let fullResponse = "";
  let retries = 0;
  const modelConfig = keyInfo?.modelConfig || {};
  while (retries <= maxRetries) {
    try {
      if (provider === 'google') {
        if (supportsStreaming) {
          const result = await model.generateContentStream(prompt);
          for await (const chunk of result.stream) {
            const text = chunk.text();
            if (text?.trim().startsWith('<')) {
  console.error("ğŸ”¥ HTML RESPONSE DETECTED");
  console.error("ğŸ”¥ Provider:", provider);
  console.error("ğŸ”¥ Model:", modelConfig?.modelName);
  console.error("ğŸ”¥ Prompt size:", prompt.length);
  throw new Error("HTML_FALLBACK_DETECTED");
}
            if (text) {
              fullResponse += text;
              onChunk(text);
            }
          }
        } else {
          const result = await model.generateContent(prompt);
          const text = result.response?.text?.() || "";
          if (text) {
            fullResponse = text;
            onChunk(text);
          }
        }
      }  else if (provider === 'deepseek' || provider === 'openrouter' || provider === 'groq') {
  const messages = [
    { role: "system", content: "You are a helpful AI coding assistant." },
    { role: "user", content: prompt }
  ];

  const apiFunction =
    provider === 'openrouter' ? callOpenRouterAPI :
    provider === 'groq'       ? callGroqAPI :
                                callDeepSeekAPI;

  // âš ï¸ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ§Ù„ ØªØ±Ø¬Ø¹ STRING Ø¯Ø§Ø¦Ù…Ù‹Ø§
  const text = await apiFunction(keyInfo, messages);

  if (!text || typeof text !== 'string') {
    throw new Error("EMPTY_MODEL_RESPONSE");
  }

  fullResponse = text;
  onChunk(text);
} else if (provider === 'kimi') {
          const kimiResponse = await callKimiAPI({
            token: keyInfo.token,
            modelConfig: keyInfo.modelConfig,
            prompt: prompt
        });
        
        if (kimiResponse) {
            fullResponse = kimiResponse;
            onChunk(kimiResponse); // Kimi Ø­Ø§Ù„ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„Ù€ streamØŒ Ù†Ø±Ø³Ù„ Ø§Ù„Ù†Øµ ÙƒØ§Ù…Ù„Ø§Ù‹
        }
      }
  
      // Ø¥Ø°Ø§ ÙˆØµÙ„Ù†Ø§ Ù‡Ù†Ø§ØŒ ÙÙ‚Ø¯ Ù†Ø¬Ø­Ù†Ø§
      onComplete(fullResponse);
      return;
      
    } catch (error) {
      retries++;
      console.error(`âŒ Attempt ${retries}/${maxRetries + 1} failed:`, error.message);
      
      // âœ… Ø¥Ø¶Ø§ÙØ© Ù‡Ø°Ø§ Ø§Ù„Ø´Ø±Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯
      if (maxRetries === 0) {
      throw error;
      }
      
      if (retries <= maxRetries) {
        console.log(`ğŸ”„ Retrying... (${retries}/${maxRetries})`);
        // Ø§Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù‚Ø¨Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
        await new Promise(resolve => setTimeout(resolve, 500));
      } else {
        // Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª ÙØ´Ù„Øª
        console.error(`âŒ All ${maxRetries + 1} attempts failed`);
        throw error;
      }
    }
  }
}

async function callKimiAPI({ token, modelConfig, prompt }) {
  const res = await fetch("https://api.moonshot.ai/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      "Authorization": `Bearer ${token}`
    },
    body: JSON.stringify({
      model: modelConfig.modelName,
      messages: [{ role: "user", content: prompt }],
      temperature: modelConfig.temperature,
      max_tokens: modelConfig.maxTokens
    })
  });

  if (!res.ok) {
    const err = await res.text();
    throw new Error(`Kimi API error: ${err}`);
  }

  const data = await res.json();
  return data.choices[0].message.content;
}

async function callGroqAPI(keyInfo, messages) {
  const res = await fetch('https://api.groq.com/openai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${keyInfo.token}`
    },
    body: JSON.stringify({
      model: keyInfo.modelConfig.modelName,
      messages,
      temperature: keyInfo.modelConfig.temperature,
      max_tokens: keyInfo.modelConfig.maxTokens,
      stream: false
    })
  });
  if (!res.ok) {
    if (res.status === 429) throw new Error('RATE_LIMITED');
    throw new Error(`Groq API error: ${res.status}`);
  }

  const data = await res.json();
  return data.choices[0].message.content;
}

// ============= Ø¯Ø§Ù„Ø© OpenRouter API Ù…Ø¹ Ø¯Ø¹Ù… Streaming Ù„Ù„Ù€ Reasoning =============
async function callOpenRouterAPI(keyInfo, messages) {
  const response = await fetch(OPENROUTER_API_URL, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${keyInfo.token}`,
      'HTTP-Referer': 'https://codeai.app',
      'X-Title': 'Codeai'
    },
    body: JSON.stringify({
      model: keyInfo.modelConfig.modelName,
      messages,
      max_tokens: keyInfo.modelConfig.maxTokens,
      temperature: keyInfo.modelConfig.temperature,
      stream: false // âœ… reasoning = no stream
    })
  });

  const raw = await response.text(); // âœ… Ø§Ù‚Ø±Ø£ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·

  if (!response.ok) {
    if (response.status === 429) {
      throw new Error('RATE_LIMITED');
    }
    throw new Error(`OpenRouter API error: ${response.status} - ${raw}`);
  }

  // ğŸ§¹ Ø£Ø­ÙŠØ§Ù†Ù‹Ø§ OpenRouter ÙŠØ±Ø¬Ø¹ data: Ø­ØªÙ‰ Ø¨Ø¯ÙˆÙ† stream
  let cleaned = raw;
  if (cleaned.startsWith("data:")) {
    cleaned = cleaned
      .split("\n")
      .filter(l => l.startsWith("data: "))
      .map(l => l.replace("data: ", ""))
      .join("");
  }

  const data = JSON.parse(cleaned);
  return data.choices[0].message.content;
}


function getFallbackModel(originalModelKey) {
    const fallbackMap = {
        // Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù€ keys Ù…Ù† MODEL_CONFIGS
        'hermes-3': 'trinity-large',
        'trinity-large': 'chimera-r1',
        'chimera-r1': 'solar-pro',
        'solar-pro': 'gpt-oss',
        'gpt-oss': 'gemini-3-flash'
    };
    
    return fallbackMap[originalModelKey] || 'gemini-3-flash';
}

/**
 * ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¹Ø¯ Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©
 */
/**
 * ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¹Ø¯ Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø© (Ù…ØµØ­Ø­)
 */
function updateUsage(keyId, modelType, tokens) {
    if (!usageStats[keyId] || !usageStats[keyId][modelType]) {
        console.error(`âŒ Invalid stats for ${keyId}.${modelType}`);
        return;
    }
    
    const stats = usageStats[keyId][modelType];
    
    // ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† tokens Ø±Ù‚Ù… ØµØ§Ù„Ø­
    const safeTokens = isNaN(parseInt(tokens)) ? 100 : parseInt(tokens);
    
    // Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù‚ÙŠÙ… Ø¥Ø°Ø§ ÙƒØ§Ù†Øª NaN
    if (isNaN(stats.rpm)) stats.rpm = 0;
    if (isNaN(stats.tpm)) stats.tpm = 0;
    if (isNaN(stats.rpd)) stats.rpd = 0;
    
    // Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØµØ­ÙŠØ­Ø©
    stats.rpm += 1;
    stats.rpd += 1;
    stats.tpm += safeTokens;
    
    console.log(`ğŸ“Š Updated ${modelType.toUpperCase()} usage for ${keyId}: RPM=${stats.rpm}, TPM=${stats.tpm}, Tokens=${safeTokens}`);
}

async function callDeepSeekAPI(keyInfo, messages) {
    const response = await fetch(keyInfo.modelConfig.apiUrl, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${keyInfo.token}`
        },
        body: JSON.stringify({
            model: keyInfo.modelConfig.modelName,
            messages: messages,
            max_tokens: keyInfo.modelConfig.maxTokens,
            temperature: keyInfo.modelConfig.temperature,
            stream: keyInfo.modelConfig.supportsStreaming
        })
    });
    
    if (!response.ok) throw new Error(`DeepSeek API error: ${response.status}`);
    return response;
}

/**
 * ØªÙ‚Ø¯ÙŠØ± Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø² Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©
 */
function estimateTokens(text) {
    if (!text || typeof text !== 'string') return 100; // Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¢Ù…Ù†Ø©
    
    // ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ: 1 ØªÙˆÙƒÙ† Ù„ÙƒÙ„ 4 Ø­Ø±ÙˆÙ (ØªÙ‚Ø±ÙŠØ¨ Gemini)
    const tokenCount = Math.ceil(text.length / 4);
    
    // ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù‚ÙŠÙ…Ø© Ø±Ù‚Ù… ØµØ­ÙŠØ­ Ù…ÙˆØ¬Ø¨
    return Math.max(100, tokenCount); // Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ 100 ØªÙˆÙƒÙ† Ù„Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø©
}

async function routeModel(userMessage) {
  const routerPrompt = `
You are a routing AI.

You MUST respond with ONLY valid JSON.
NO explanations.
NO markdown.
NO extra text.

Response format (STRICT):
{"recommended_model":"MODEL_ID"}

Available models:

1) gemini-3-flash
- Best for: fast replies, casual chat, simple questions
- Strengths: very fast, low cost
- Weaknesses: limited deep reasoning

2) gemini-2.5-pro
- Best for: deep reasoning, analysis, complex logic
- Strengths: strong thinking and planning
- Weaknesses: slower than flash

3) gemini-2.5-flash
- Best for: balanced reasoning and speed
- Strengths: good general-purpose model
- Weaknesses: not best for heavy coding

4) qwen-coder
- Best for: coding, debugging, refactoring, file edits
- Strengths: excellent code understanding
- Weaknesses: slower for casual chat

5) chimera-r1
- Best for: research-level reasoning, multi-step analysis
- Strengths: very deep thinking
- Weaknesses: slower, higher cost

6) hermes-3
- Best for: long conversations, explanations, structured output
- Strengths: strong instruction following
- Weaknesses: not code-specialized

7) gpt-oss
- Best for: general knowledge, explanations
- Strengths: stable, predictable
- Weaknesses: weaker than top-tier reasoning

8) solar-pro
- Best for: multilingual chat, balanced tasks
- Strengths: good Arabic + English
- Weaknesses: not best at heavy reasoning

9) trinity-large
- Best for: very complex reasoning and planning
- Strengths: extremely powerful
- Weaknesses: slow, expensive

Rules:
- Choose ONLY from the listed model_id values.
- Prefer specialized models over general ones.
- If unsure, choose gemini-3-flash.

User message:
"${userMessage}"
`;

  const genAI = new GoogleGenerativeAI(getSafeKeyForModel("gemma").token);

  const model = genAI.getGenerativeModel({
    model: "gemma-3-27b-it",
    generationConfig: {
      temperature: 0,
      maxOutputTokens: 50
    }
  });

  const result = await model.generateContent(routerPrompt);
  

  let text = result.response.text().trim();

// Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON ÙÙ‚Ø·
const jsonMatch = text.match(/\{[\s\S]*\}/);

if (!jsonMatch) {
  console.error("âŒ No JSON found in router response:", text);
  return { recommended_model: "gemini-3-flash" };
}

try {
  return JSON.parse(jsonMatch[0]);
} catch (e) {
  console.error("âŒ Router JSON parse failed:", jsonMatch[0]);
  return { recommended_model: "gemini-3-flash" };
}
}

async function summarizeConversationWithGemma(convId, userMessage, aiResponse) {
  console.log(`\nğŸ¯ ===== GEMMA SUMMARIZATION STARTED =====`);
  
  // Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Ù„Ù€ Gemma
  const gemmaKeyInfo = getSafeKey('gemma');

  if (!gemmaKeyInfo) {
    console.warn("âš ï¸ No available keys for Gemma summarization");
    console.log(`ğŸ”„ Using fallback summary`);
    const fallback = generateFallbackSummary(userMessage);
    console.log(`ğŸ“ Fallback summary: "${fallback}"`);
    console.log(`âŒ ===== GEMMA SUMMARIZATION SKIPPED =====\n`);
    return fallback;
  }

  console.log(`ğŸ”‘ Using Gemma Key: ${gemmaKeyInfo.id}`);
  console.log(`ğŸ“Š Current Gemma usage: RPM=${usageStats[gemmaKeyInfo.id]?.gemma?.rpm || 0}, TPM=${usageStats[gemmaKeyInfo.id]?.gemma?.tpm || 0}`);

  try {
    const summaryPrompt = `You are an AI conversation summarizer. Your ONLY task is to generate a short, descriptive title for a coding/development conversation.

STRICT RULES:
1. Read ONLY the user's first message and the AI's first response
2. Generate a concise, descriptive title (MAX 40 characters)
3. Use the same language as the conversation (English or Arabic)
4. DO NOT add quotes, punctuation, or emojis
5. DO NOT use "..." truncation - make it a complete phrase
6. DO NOT mention "conversation about" or "discussion of"
7. Make it natural like app conversation titles
8. Focus on the main task/request

CONVERSATION:
User: ${userMessage}
AI: ${aiResponse.substring(0, 200)}

TITLE:`;

    console.log(`ğŸ“‹ Summary prompt length: ${summaryPrompt.length} chars`);
    console.log(`ğŸ“‹ Prompt preview: ${summaryPrompt.substring(0, 150)}...`);

    const response = await fetch(
      'https://generativelanguage.googleapis.com/v1beta/models/gemma-3-1b-it:generateContent',
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-goog-api-key': gemmaKeyInfo.token
        },
        body: JSON.stringify({
          contents: [{
            parts: [{ text: summaryPrompt }]
          }],
          generationConfig: {
            maxOutputTokens: 50,
            temperature: 0.3,
            topP: 0.9,
            topK: 40
          }
        })
      }
    );

    if (!response.ok) {
      const errorText = await response.text();
      console.error(`âŒ Gemma API error: ${response.status} - ${errorText.substring(0, 200)}`);
      throw new Error(`Gemma API error: ${response.status}`);
    }

    const data = await response.json();
    const summary = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
    
    console.log(`ğŸ“ Raw summary from Gemma: "${summary}"`);
    
    // ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ„Ø®ÙŠØµ
    const cleanedSummary = cleanSummary(summary, userMessage);
    
    // ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
    const tokens = estimateTokens(summaryPrompt + cleanedSummary);
    updateUsage(gemmaKeyInfo.id, 'gemma', tokens);
    
    console.log(`âœ… Cleaned summary: "${cleanedSummary}"`);
    return cleanedSummary;
    
  } catch (error) {
    console.error("âŒ Gemma summarization failed:", error);
    // Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¯ÙŠÙ„ Ù…Ø­Ø³Ù†
    
  }
}

function cleanSummary(summary, userMessage) {
  console.log(`ğŸ§¹ Cleaning summary: "${summary}"`);
  
  if (!summary || summary.trim().length === 0) {
    console.log(`ğŸ§¼ Empty summary, using fallback`);
    
  }
  
  // Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ†ØµÙŠØµ ÙˆØ§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©
  let cleaned = summary
    .trim()
    .replace(/^["'`]|["'`]$/g, '')  // Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ†ØµÙŠØµ
    .replace(/^[\[\]\(\)]|[\[\]\(\)]$/g, '') // Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù‚ÙˆØ§Ø³
    .replace(/\.\.\.$/g, '') // Ø¥Ø²Ø§Ù„Ø© "..." Ù…Ù† Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
    .replace(/\s+/g, ' ')    // Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
    .replace(/^\d+\.\s*/, '') // Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
    .trim();
  
  console.log(`ğŸ§¹ After initial clean: "${cleaned}" (${cleaned.length} chars)`);
  
  // Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø·ÙˆÙ„ ÙˆØªØ­Ø³ÙŠÙ†Ù‡
  if (cleaned.length > 40) {
    // Ù…Ø­Ø§ÙˆÙ„Ø© Ø°ÙƒÙŠØ© Ù„Ù„ØªÙ‚ØµÙŠØ± Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… "..."
    cleaned = smartTruncate(cleaned, 40);
    console.log(`âœ‚ï¸ Smart truncated: "${cleaned}" (${cleaned.length} chars)`);
  }
  
  // Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‚ØµÙŠØ±Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ Ø£Ùˆ ÙØ§Ø±ØºØ§Ù‹
  if (cleaned.length < 3) {
    
  }
  
  // ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø£ÙˆÙ„ ÙƒØ¨ÙŠØ± (Ù„Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)
  if (/^[a-z]/.test(cleaned)) {
    cleaned = cleaned.charAt(0).toUpperCase() + cleaned.slice(1);
  }
  
  console.log(`âœ… Final cleaned: "${cleaned}"`);
  return cleaned;
}

/**
 * ØªÙ‚ØµÙŠØ± Ø°ÙƒÙŠ Ù„Ù„Ù†Øµ Ø¯ÙˆÙ† Ù‚Ø·Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
 */
function smartTruncate(text, maxLength) {
  if (text.length <= maxLength) return text;
  
  // Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³Ø§ÙØ© Ù„Ù„Ù‚Ø·Ø¹ Ø¹Ù†Ø¯Ù‡Ø§
  let truncated = text.substring(0, maxLength);
  const lastSpace = truncated.lastIndexOf(' ');
  
  if (lastSpace > maxLength * 0.7) { // Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ù…Ø³Ø§ÙØ© Ù…Ù†Ø§Ø³Ø¨Ø©
    truncated = truncated.substring(0, lastSpace);
  }
  
  // Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ ÙÙˆØ§ØµÙ„ Ø²Ø§Ø¦Ø¯Ø© ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
  truncated = truncated.replace(/[,\-\:;\.\s]+$/, '');
  
  return truncated;
}

/**
 * Ø¯Ø§Ù„Ø© Ø¨Ø¯ÙŠÙ„Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ ØªÙ„Ø®ÙŠØµ ÙÙŠ Ø­Ø§Ù„Ø© ÙØ´Ù„ API
 */
function generateFallbackSummary(userMessage) {
    // Ù„Ø®Øµ Ø£ÙˆÙ„ Ø±Ø³Ø§Ù„Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    let summary = userMessage
        .replace(/[<>]/g, '') // Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª HTML
        .replace(/\n/g, ' ')  // Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø£Ø³Ø·Ø± Ø¨Ù…Ø³Ø§ÙØ§Øª
        .trim();
    
    // Ù‚Ø·Ø¹ Ù„Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
    if (summary.length > 40) {
        summary = summary.substring(0, 37) + '...';
    }
    
    // Ø¥Ø°Ø§ ÙƒØ§Ù† ÙØ§Ø±ØºØ§Ù‹ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ù†ÙˆØ§Ù† Ø§ÙØªØ±Ø§Ø¶ÙŠ
    if (!summary || summary.length < 3) {
        return "New Conversation";
    }
    
    return summary;
}



const app = express();
app.use(cors());
app.use(bodyParser.json({ limit: '10mb' })); // Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø­Ø¯ Ù„Ø§Ø³ØªÙŠØ¹Ø§Ø¨ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©

app.use(express.static(path.join(__dirname, '..', 'client')));

let clients = [];
let conversationMemory = {};

// ==========================================
// 1. ØªØ­Ø¯ÙŠØ« Ø¯Ø§Ù„Ø© broadcast Ù„ØªÙ‚Ø¨Ù„ targetClientId
// ==========================================
function broadcast(data, targetClientId = null) {
  const msg = `data: ${JSON.stringify(data)}\n\n`;
  clients.forEach(c => {
    // Ù†Ø±Ø³Ù„ ÙÙ‚Ø· Ù„Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø°ÙŠ ÙŠØ·Ø§Ø¨Ù‚ Ø§Ù„Ù€ clientId
    // Ø£Ùˆ Ù†Ø±Ø³Ù„ Ù„Ù„Ø¬Ù…ÙŠØ¹ Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ù‡Ø¯Ù (Ù„Ù„ØªÙˆØ§ÙÙ‚ØŒ Ù„ÙƒÙ† ÙŠÙØ¶Ù„ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø§Ù„ØªØ­Ø¯ÙŠØ¯)
    if (!targetClientId || c.clientId === targetClientId) {
      try { c.res.write(msg); }
      catch(e) { console.error("âŒ Broadcast error:", e); }
    }
  });
}

function getNextFallback(station, currentModel) {
  const list = STATION_MAP[station];
  const index = list.indexOf(currentModel);
  return list[index + 1] || null;
}

function sendStageUpdate(clientId, userText, modelId, actionText) {
  const modelName = MODEL_CONFIGS[modelId]?.displayName || modelId || "AI";

  broadcast({
    type: "assistant_message",
    stage: true,
    text: `${userText}\n${modelName} ${actionText}`
  }, clientId);
}

async function classifyTaskAndThinker(userMessage, files) {
  const prompt = `
You are a task analysis AI.

Your job:
1) Classify the user intent.
2) Decide if deep reasoning is needed.
3) Select the best reasoning model.
4) If the task is FIX, identify where the problem is.

Return ONLY valid JSON in this exact format:
{
  "intent": "build | fix | improve | refactor | explain",
  "needs_reasoning": true | false,
  "reasoning_model": "MODEL_ID or null",
  "fault": {
    "type": "logic | syntax | performance | ui | unknown",
    "files": [],
    "location": "",
    "summary": ""
  }
}

Rules:
- If intent is NOT "fix", set fault = null
- Be concise.
- Do NOT solve the problem.
- Do NOT write code.

Available models: 
gemini-3-flash
gemini-2.5-pro
gemini-2.5-flash
trinity-large
chimera-r1
hermes-3
gpt-oss
solar-pro



User message:
"${userMessage}"
`;

  const genAI = new GoogleGenerativeAI(
    getSafeKeyForModel("gemini-3-flash").token
  );

  const model = genAI.getGenerativeModel({
    model: "gemini-3-flash-preview",
    generationConfig: {
      temperature: 0,
      maxOutputTokens: 150
    }
  });

  const result = await model.generateContent(prompt);
  const text = result.response.text().trim();

  const match = text.match(/\{[\s\S]*\}/);
  if (!match) {
    return {
      intent: "explain",
      needs_reasoning: false,
      reasoning_model: null
    };
  }

  try {
    return JSON.parse(match[0]);
  } catch {
    return {
      intent: "explain",
      needs_reasoning: false,
      reasoning_model: null
    };
  }
}

async function internalReasoning(taskInfo, message, files) {
  
  const filesContext = files?.length
  ? files.map(f => `FILE: ${f.name}\n${f.content}`).join("\n\n")
  : "";

  const prompt = `
You are a senior software architect and developer.

Your task is to:
1. Analyze the user's request DEEPLY
2. Plan a COMPLETE implementation
3. Suggest SMART additional features
4. Write clear execution instructions

Return ONLY valid JSON.
Do NOT add explanations.
Do NOT add markdown.
Do NOT add bullet points.
Do NOT add newlines.

All values MUST be single-line strings.

JSON format (exact):
{
  "internal_analysis": "DETAILED TECHNICAL PLAN for executor",
  "user_explanation": "problem=... | cause=... | solution=... | result=... | features=..."
}



IMPORTANT - For BUILD tasks:
- Plan COMPLETE implementation, not just template
- Suggest 2-3 additional features the user might like
- Specify exactly what files to create
- Include game mechanics, scoring, levels if relevant
- Make it feel like a finished product, not a starter

Task intent: ${taskInfo.intent}
User message: "${message}"

${filesContext}
`;
  const modelId = taskInfo.reasoning_model;
  const modelConfig = MODEL_CONFIGS[modelId];

  if (!modelConfig) {
    throw new Error(`Reasoning model not found: ${modelId}`);
  }

  const keyInfo = getSafeKeyForModel(modelId);
  if (!keyInfo) {
    throw new Error("No available key for reasoning model");
  }

  let text = "";

  // ğŸ”¹ Google
  if (modelConfig.provider === "google") {
    const genAI = new GoogleGenerativeAI(keyInfo.token);
    const model = genAI.getGenerativeModel({
      model: modelConfig.modelName,
      generationConfig: {
        temperature: 0.2,
        maxOutputTokens: 800
      }
    });

    const result = await model.generateContent(prompt);
    text = result.response.text().trim(); 
    
    
    
    // âœ… Ù„Ø§ ØªØ­ÙˆÙ„ \n Ø¥Ù„Ù‰ \\n Ù‡Ù†Ø§! Ù‡Ø°Ø§ ÙŠØ³Ø¨Ø¨ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
    // ÙÙ‚Ø· ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ JSON ØµØ­ÙŠØ­
    console.log("ğŸ“¦ response preview:", text);
  }

  // ğŸ”¹ OpenRouter
  if (modelConfig.provider === "openrouter") {
    const messages = [
      { role: "system", content: "You are a senior software architect." },
      { role: "user", content: prompt }
    ];

    const result = await callOpenRouterAPI(keyInfo, messages);
    text = result.trim();
    console.log("ğŸ“¦ Raw OpenRouter response length:", text.length);
        console.log("ğŸ“¦ Response preview:", text);
        text = text
      .replace(/```json\s*/g, '')  // Ø¥Ø²Ø§Ù„Ø© ```json
      .replace(/```\s*$/g, '')      // Ø¥Ø²Ø§Ù„Ø© ``` ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
      .trim();
    
    // âœ… Ù„Ø§ ØªØ­ÙˆÙ„ \n Ø¥Ù„Ù‰ \\n Ù‡Ù†Ø§! Ù‡Ø°Ø§ ÙŠØ³Ø¨Ø¨ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
    // ÙÙ‚Ø· ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ JSON ØµØ­ÙŠØ­
console.log("ğŸ“¦ Cleaned preview:", text);
    if (text?.trim().startsWith('<')) {
      console.error("ğŸ”¥ HTML RESPONSE DETECTED");
      throw new Error("HTML_FALLBACK_DETECTED");
    }
  }

  if (!text) {
    throw new Error("Reasoning model returned empty response");
  }

  // ğŸ”§ FIXED: Better JSON extraction and parsing
  const cleaned = text
    .replace(/```json\s*/g, "")
    .replace(/```\s*$/g, "")
    .trim();

  // Try to find JSON object in the response
  const jsonMatch = cleaned.match(/\{[\s\S]*\}/);
  
  if (!jsonMatch) {
    console.log("âš ï¸ No JSON found in reasoning response, creating fallback structure");
    console.log("Raw text:", text.substring(0, 200) + "...");
    
    return {
  internal_analysis: "Direct execution without detailed reasoning.",
  user_explanation: generateFallbackExplanation(taskInfo, message)  // âœ… Ù†Øµ ÙˆÙ„ÙŠØ³ ÙƒØ§Ø¦Ù†
};
  }

  try {
    const parsed = JSON.parse(jsonMatch[0]);
    
    // Validate and ensure structure
    return {
  internal_analysis: parsed.internal_analysis || "No analysis provided",
  user_explanation: parsed.user_explanation || generateFallbackExplanation(taskInfo, message)
};
  } catch (err) {
    console.error("âŒ Failed to parse reasoning JSON:", err.message);
    console.log("JSON string attempted:", jsonMatch[0].substring(0, 200) + "...");
    
    try {
    // Ù…Ø­Ø§ÙˆÙ„Ø© 2: Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù€ newlines Ø¯Ø§Ø®Ù„ Ø§Ù„Ù€ string
    const fixed = jsonMatch[0]
      .replace(/\n/g, '\\n')
      .replace(/\r/g, '\\r')
      .replace(/\t/g, '\\t');
    
    const parsed = JSON.parse(fixed);
    return {
      internal_analysis: parsed.internal_analysis || "",
      user_explanation: parsed.user_explanation || generateFallbackExplanation(taskInfo, message)
    };
  } catch (err2) {
    console.error("âŒ All JSON parsing attempts failed");
    
    // Ù…Ø­Ø§ÙˆÙ„Ø© 3: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ ÙŠØ¯ÙˆÙŠØ§Ù‹ (Ø¢Ø®Ø± Ø£Ù…Ù„)
    const extracted = extractTextFromBrokenJSON(jsonMatch[0]);
    return {
      internal_analysis: extracted.analysis || "Failed to parse reasoning response",
      user_explanation: extracted.explanation || generateFallbackExplanation(taskInfo, message)
    };
  }
    
    
  }
}

function formatUserExplanation(text) {
  if (!text || typeof text !== "string") return "";

  const parts = text.split("|").map(p => p.trim());

  return parts
    .map(p => {
      const [key, ...rest] = p.split("=");
      if (!rest.length) return null;
      return `â€¢ ${key.trim()}: ${rest.join("=").trim()}`;
    })
    .filter(Boolean)
    .join("\n");
}


function isAutoMode(modelId) {
    return modelId === "auto" 
        || modelId === "codeai-code-r";
}

async function executeWithFallback({
  station,
  prompt,
  clientId,
  onChunk,
  onComplete
}) {
  const models = STATION_FALLBACKS[station];
  if (!models || !models.length) {
    throw new Error(`NO_MODELS_FOR_STATION:${station}`);
  }

  const tried = new Set();

  for (const modelId of models) {
    if (tried.has(modelId)) continue;
    tried.add(modelId);

    const modelConfig = MODEL_CONFIGS[modelId];
    if (!modelConfig) continue;

    const keyInfo = getSafeKeyForModel(modelId);
    if (!keyInfo) continue;

    if (!isProviderAvailable(modelConfig.provider)) continue;

    try {
      sendStageUpdate(
        clientId,
        "Processing request",
        modelId,
        "is running"
      );

      await sendResponseUnified({
        model:
          modelConfig.provider === "google"
            ? new GoogleGenerativeAI(keyInfo.token).getGenerativeModel({
                model: modelConfig.modelName,
                generationConfig: {
                  maxOutputTokens: modelConfig.maxTokens,
                  temperature: modelConfig.temperature
                }
              })
            : null,
        provider: modelConfig.provider,
        keyInfo,
        prompt,
        supportsStreaming: modelConfig.supportsStreaming,
        onChunk,
        onComplete,
        maxRetries: 0
      });

      return; // âœ… Ù†Ø¬Ø§Ø­ = Ø®Ø±ÙˆØ¬ Ù†Ù‡Ø§Ø¦ÙŠ

    } catch (err) {
      console.warn(`âŒ ${modelId} failed: ${err.message}`);

      // â›” Ø­Ø¸Ø± Ø§Ù„Ù…Ø²ÙˆØ¯ Ø¹Ù†Ø¯ 429
      if (err.message === 'RATE_LIMITED') {
        markProviderRateLimited(modelConfig.provider);
      }

      // â¬‡ï¸ Ù†ÙƒÙ…Ù„ Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ§Ù„ÙŠ
      continue;
    }
  }

  throw new Error("ALL_FALLBACK_MODELS_FAILED");
}

app.post('/api/chat', async (req, res) => {
  // 1. Ù†Ø³ØªÙ‚Ø¨Ù„ Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ÙƒÙˆØ¯ ÙˆØ§Ø­Ø¯
const { message, files, convId, history, settings, clientId } = req.body;
const requestedModel = settings?.selectedModel || 'gemini-3-flash';
const isAutoPro = requestedModel === "codeai-code-r";
let finalModel = requestedModel;
let taskInfo = null;
const modelConfig =
  MODEL_CONFIGS[finalModel] || MODEL_CONFIGS['gemini-3-flash'];

if (requestedModel === "auto") {
  const route = await routeModel(message);
  finalModel = route.recommended_model;
  console.log("Final model selected :", finalModel)
};
if (requestedModel === "codeai-code-r") {
  taskInfo = await classifyTaskAndThinker(message, files);
  sendStageUpdate(
  clientId,
  "Analyzing your request..",
  "gemini-3-flash",
  "is analyzing.."
);
if (isAutoPro) {


  if (taskInfo.intent === "explain") {
    taskInfo.intent = "build";
  }


  taskInfo.needs_reasoning = true;

  // 3ï¸âƒ£ Ø¶Ù…Ø§Ù† ÙˆØ¬ÙˆØ¯ Ù†Ù…ÙˆØ°Ø¬ ØªÙÙƒÙŠØ±
  if (!taskInfo.reasoning_model) {
    taskInfo.reasoning_model = "trinity-large";
  }
}

  console.log("ğŸ§  code-R analysis:", taskInfo);
}
let reasoningData = null;
let reasoningSummary = null;
let userExplanation = null;
let combinedModelName = null;

if (taskInfo?.needs_reasoning) {
  sendStageUpdate(
    clientId,
    "Identifying the problem..",
    taskInfo.reasoning_model,
    "is thinking.."
  );
}

// ğŸ”¸ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
if (taskInfo && taskInfo.needs_reasoning) {
  try {
    sendStageUpdate(
      clientId,
      "Identifying the problem..",
      taskInfo.reasoning_model,
      "is thinking.."
    );

    reasoningData = await internalReasoning(taskInfo, message, files);
    
    if (reasoningData) {
      reasoningSummary = reasoningData.internal_analysis;
      userExplanation = reasoningData.user_explanation;
      const displayExplanation =
  formatUserExplanation(userExplanation);
  
      broadcast({
        type: 'thought_process',
        text: displayExplanation || "No analysis details provided."
      }, clientId);
    }

  } catch (err) {
    console.warn("âš ï¸ Reasoning failed:", err.message);
  }
}

// ğŸ”¸ Fallback ØªÙ„Ù‚Ø§Ø¦ÙŠ
if (taskInfo && !reasoningData && taskInfo.needs_reasoning) {
  console.log("ğŸ” Reasoning fallback â†’ Gemini Flash");

  

  try {
    sendStageUpdate(
      clientId,
      "Identifying the problem..",
      taskInfo.reasoning_model,
      "is thinking.."
    );

    let currentModel = taskInfo ? taskInfo.reasoning_model : null;

reasoningData = await executeWithFallback({
  station: "B",
  prompt: message,
  clientId,
  onChunk: () => {},
  onComplete: (full) => full
});
} catch (error) {
  /* handle error */
  }


// ğŸ”¸ Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ØªÙˆÙ‚Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
if (!reasoningData) {
  reasoningData = {
    internal_analysis: "",
    user_explanation: {
      problem: "",
      cause: "",
      solution: "Proceeding directly with execution.",
      result: ""
    }
  };
}
  

const reasoningModelName = (taskInfo && taskInfo.reasoning_model) 
    ? MODEL_CONFIGS[taskInfo.reasoning_model]?.displayName || "Trinity large"
    : "Trinity large";
 combinedModelName = `${reasoningModelName} + ${modelConfig.displayName}`;

  

  if (reasoningData) {
    reasoningSummary = reasoningData.internal_analysis;
    userExplanation = reasoningData.user_explanation;

    // >>>> Ø¥Ø¶Ø§ÙØ©: Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®ØªØµØ± ÙÙˆØ±Ø§Ù‹ Ù„Ù„Ø¹Ù…ÙŠÙ„ <<<<
    broadcast({
        type: 'thought_process',
        text: reasoningSummary || "No analysis details provided."
    }, clientId);
  }
}
 


let finalKeyInfo = null;

if (!isAutoMode(finalModel)) {
    finalKeyInfo = getSafeKeyForModel(finalModel);
}
let usedModelName = modelConfig.displayName;
let provider = modelConfig.provider;
  
  
if (requestedModel === "auto" || requestedModel === "codeai-code-r") {
    const routeResult = await routeModel(message, files);

    finalModel = routeResult.recommended_model; // Ù…Ø«Ù„ qwen-coder
    console.log("reasoning model:", finalModel)
    const candidateProvider = MODEL_CONFIGS[finalModel].provider;

if (!isProviderAvailable(candidateProvider)) {
  throw new Error(`PROVIDER_BLOCKED:${candidateProvider}`);
}

provider = candidateProvider;

    finalKeyInfo = getSafeKeyForModel(finalModel);
}

    console.log(`ğŸ¯ Requested model: ${modelConfig.displayName} (Provider: ${modelConfig.provider})`);
console.log(`ğŸ”‘ Using Key: ${finalKeyInfo} for ${provider}`);
    
const optimizedHistory = history.map((msg, index) => {
    if (index >= history.length - 2) {
        return { ...msg, files: [] }; // Ø¥ÙØ±Ø§Øº Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ø¢Ø®Ø± Ø±Ø³Ø§Ù„ØªÙŠÙ†
    }
    
    return msg;
});
console.log("optimizedHistory:", optimizedHistory)



if (!conversationMemory[convId]) {
    conversationMemory[convId] = {
        summary: "",
        history: [], // Ù‡Ø°Ø§ Ø³ÙŠØ®Ø²Ù† Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„ÙƒÙ„ Ø±Ø¯ (Ù„ÙŠØ³ chunks)
        messageCount: 0 // ğŸ‘ˆ Ø¥Ø¶Ø§ÙØ© Ø¹Ø¯Ø§Ø¯ Ù„Ù„Ø±Ø³Ø§Ø¦Ù„
    };
}

const activeKeyInfo = getSafeKey();
    
      // Ù…Ù†Ø·Ù‚ Ø§Ù„Ù€ Fallback (Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø®ØªØ§Ø± Ù†Ø¹ÙˆØ¯ Ù„Ù€ Gemini Flash)
  if (!finalKeyInfo) {
      console.log(`âš ï¸ Primary model keys exhausted. Switching to Fallback...`);
      finalKeyInfo = getSafeKey('gemini'); 
      
      if (!finalKeyInfo) {
          broadcast({ type: 'assistant_message', text: 'âš ï¸ Ø§Ù„Ø³ÙŠØ±ÙØ± Ù…Ø´ØºÙˆÙ„ Ø¬Ø¯Ø§Ù‹.' }, clientId);
          return res.json({ status: 'limit-reached' });
      }
       // ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§Ø³Ù… ÙŠØ¯ÙˆÙŠØ§Ù‹ Ù„Ù„Ù€ Fallback
      usedModelName = "Gemini 3 Flash (Auto)"; 
      provider = 'google';
      
      // Ø¥Ø¹Ø¯Ø§Ø¯ config ÙˆÙ‡Ù…ÙŠ Ù„Ù„Ù€ fallback
      finalKeyInfo.modelConfig = {
          modelName: 'gemini-2.5-flash', // Ø£Ùˆ Ø§Ù„Ù…ØªØ§Ø­ Ù„Ø¯ÙŠÙƒ
          maxTokens: 100000,
          temperature: 0.7,
          supportsStreaming: true
      };
  } else {
      // Ø¥Ø°Ø§ Ù†Ø¬Ø­Ù†Ø§ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø±Ø³Ù…ÙŠ
      usedModelName = finalKeyInfo.modelConfig.displayName;
  }


console.log(`ğŸ¯ ===== REQUEST STARTED =====`);
console.log(`ğŸ“Œ Conversation ID: ${convId}`);
console.log(`ğŸ”‘ Using Key: ${activeKeyInfo.id}`);

console.log(`ğŸ“ User message: ${message.substring(0, 100)}...`);

  

  try {
    let model;
       if (provider === 'google') {
       // Ø¥Ø¹Ø¯Ø§Ø¯ Gemini
       const genAI = new GoogleGenerativeAI(finalKeyInfo.token);
       model = genAI.getGenerativeModel({ 
           model: finalKeyInfo.modelConfig.modelName,
           generationConfig: { 
               maxOutputTokens: finalKeyInfo.modelConfig.maxTokens,
               temperature: finalKeyInfo.modelConfig.temperature
           }
       });
    }

    // Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© ÙØ§Ø±ØºØ© Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ù€ Stream Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙÙ‚Ø·
   // broadcast({ type: 'assistant_message', text: ' ' }, clientId);
    
    

const estimatedRequestTokens = estimateTokens(message + JSON.stringify(files || ""));

    // ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª (Ø¨Ø´ÙƒÙ„ Ù…Ø¤Ù‚Øª Ù‚Ø¨Ù„ Ø§Ù„Ø·Ù„Ø¨)
        usageStats[activeKeyInfo.id].gemini.rpm += 1;
usageStats[activeKeyInfo.id].gemini.rpd += 1;
usageStats[activeKeyInfo.id].gemini.tpm += estimatedRequestTokens;

    

    // 1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¨ØµØ±ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø«ÙŠÙ… (Dark/Light)
    let visualStyleInstruction = "";
    if (settings && settings.theme === 'light') {
        visualStyleInstruction = `
- (LIGHT THEME)
If the user does not specify a particular design or theme, ALWAYS apply the following default style:
1. Colors:
   - Background: #FFFFFF (Pure White)
   - Secondary/Surface: #E0E0E0
   - Text: #080808 (Deep Black)
   - Accent: #CCCCCC
   - Borders: rgba(0,0,0,0.1)
2. Components:
   - Use distinct shadows (box-shadow: 0 2px 8px rgba(0,0,0,0.05)) for cards.
   - Buttons: Black text on White background or Light Grey.
   - Modals, Cards, and Menus: border-radius: 16px;
   - Buttons: border-radius: 30px; background-color: #000000; color: #080808; (Change colors only if multiple buttons exist to show hierarchy).
3. Typography:
   - For English text, use the 'Archives' font family.
`;
    } else {
        // Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¸Ù„Ù… (Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø§Ù„Ù‚Ø¯ÙŠÙ…)
        visualStyleInstruction = `
- (DARK THEME)
If the user does not specify a particular design or theme, ALWAYS apply the following default style:
1. Colors:
   - Background: #080808 (Deep Black)
   - Secondary/Surface: #2A2A2A
   - Text: #FFFFFF (Pure White)
   - Accent: #333333
2. Typography:
   - For English text, use the 'Archives' font family.
3. Components:
   - Modals, Cards, and Menus: border-radius: 16px;
   - Buttons: border-radius: 30px; background-color: #FFFFFF; color: #000000; (Change colors only if multiple buttons exist to show hierarchy).
`;
    }

    // 2. ØªØ­Ø¯ÙŠØ¯ Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ (Detailed vs Simple)
    let personaInstruction = "";
    if (settings && settings.convStyle === 'Simple') {
        personaInstruction = `
- COMMUNICATION STYLE: SIMPLE & INTERACTIVE -
You are chatting with a non-technical user or someone who wants quick results.
1. DO NOT explain the code in detail.
2. DO NOT list changed files unless asked.
3. Just say enthusiastically: "I've updated the design for you!", "Game is ready!", etc.
4. Be very interactive, ask "Do you want to change the colors?", "Shall we add sound?".
`;
    } else {
        // Detailed (Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ)
        personaInstruction = `
- COMMUNICATION STYLE: DETAILED & EXPERT -
You are chatting with a developer.
1. Briefly explain the technical changes.
2. Be interactive but professional.
`;
    }

    // 3. Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙØ¶Ù„Ø©
    const prefLang = settings && settings.prefLanguage ? settings.prefLanguage : 'HTML';

    // 2. ØªØ­Ø¶ÙŠØ± Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ø¥Ø±Ø³Ø§Ù„Ù‡ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
    let filesContext = "";
    if (files && Array.isArray(files)) {
        filesContext = files.map(f => 
            `--- FILE START: ${f.name} ---\n${f.content}\n--- FILE END: ${f.name} ---`
        ).join("\n\n");
    }
    let taskModeInstruction = "";

if (taskInfo) {
  switch (taskInfo.intent) {
    case "build":
      taskModeInstruction = `
- TASK MODE: BUILD -
You are creating new features or a new project.
Focus on structure, clarity, and completeness.
`;
      break;

    case "fix":
      taskModeInstruction = `
- TASK MODE: FIX -
You are fixing a bug.
Make minimal, targeted changes.
Do NOT refactor unless necessary.
`;
      break;

    case "improve":
      taskModeInstruction = `
- TASK MODE: IMPROVE -
Enhance existing functionality without breaking behavior.
`;
      break;

    case "refactor":
      taskModeInstruction = `
- TASK MODE: REFACTOR -
Improve code quality and structure without changing functionality.
`;
      break;
  }
}
// 3. ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    const systemInstruction = `You are an expert, friendly web developer.




--------------------------------------------------
GLOBAL IDENTITY
--------------------------------------------------
--- INFO ---

- YOUR GOAL -
Help the user by editing existing files or CREATING new files based on their request.
- ABOUT -
1. Identity & Platform:
You are Codeai (in arabic (ÙƒÙˆØ¯Ø§ÙŠ)), an integrated AI chat assistant and code editor. You operate within the Codeai PWA, designed to provide a seamless coding and assistance experience.
2.â€‹Capabilities & Constraints:
You support code generation and live previews for the following languages only: HTML, CSS, JavaScript, Java, Python(console view only). Ensure all technical solutions and previews align with these supported environments.
3. If the user asks for anything that doesn't relate to coding, answer them normally; you aren't for coding only.

--------------------------------------------------
USER CONTEXT
--------------------------------------------------
- Preferred Language: ${prefLang} (Default to this if starting a new project).
- Theme: ${settings?.theme || 'dark'}
--------------------------------------------------
DESIGN CONTEXT (if UI involved)
--------------------------------------------------
${visualStyleInstruction}
- ALWAYS include the following block at the very beginning of every CSS file or <style> tag:
* {
    -webkit-tap-highlight-color: transparent;
}
- NEVER use alert(), Make your own modal instead.
- Try always to add simple animations for buttons, modals, cards, and almost everything that makes the app/game better

${personaInstruction}
--------------------------------------------------
RULES
--------------------------------------------------
1. **Language:** Reply in the language the user speaks.
2. **Multi-File Capability:** You can edit multiple files in one response.
3. To ADD new functions/classes (without repeating code): 
   Use <ADD_TO target="filename.ext" position="end">content</ADD_TO> (position can be "start" or "end").
4. For SMALL changes: Use <REPLACE file="filename.ext">
   <<<<<<< SEARCH
   one or two lines ONLY to find
   =======
   new lines
   >>>>>>> REPLACE
   </REPLACE>
5. **New Files:** If the user asks for a new file, output a file block with that name.
6. You can provide multiple <FILE> or <ADD_TO> or <REPLACE> blocks in a single response if the task requires changing multiple files (e.g., updating HTML, CSS, and JS together)."
7. â€‹Dumping & Coding: Place all diffs and code blocks at the absolute end. Ensure any conversational text or questions for the user precede the code markers <>, as anything following them is hidden.
8. NEVER output templates
9. ALWAYS deliver a complete, polished product
10. ADD 2-3 extra features the user didn't ask for but would love
11. MAKE IT FEEL FINISHED, not like a starter
--- OUTPUT FORMAT (STRICT) ---
To create a file, use this EXACT format at the end of your response:

<FILE name="filename.ext">
... FULL code content here ...
</FILE>

${taskModeInstruction}
`;

 // 5. Ø¯Ù…Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ® (Context)
    // Ø§Ø¨Ø­Ø« Ø¹Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ ÙÙŠ server.js ÙˆØ¹Ø¯Ù„Ù‡ Ù„ÙŠØµØ¨Ø­ Ù‡ÙƒØ°Ø§:
let historyText = "";
if (history && Array.isArray(history)) {
    historyText = history.map(msg => {
        // ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø­Ù‚Ù„ Ø§Ù„ØµØ­ÙŠØ­ (sender Ø£Ùˆ role)
        const role = msg.role || msg.sender || 'user'; 
        const text = msg.text || msg.content || '';
        return `[${role.toUpperCase()}]: ${text.substring(0, 500)}`;
    }).join("\n");
}




let internalGuidance = "";

if (reasoningSummary) {
  internalGuidance = `
--- INTERNAL GUIDANCE (DO NOT EXPOSE TO USER) ---
${reasoningSummary}
`;
}
let executionContext =''

if (taskInfo) {
executionContext = `
--- EXECUTION TASK ---
You are fixing an existing issue.
Your job is to APPLY changes exactly as instructed.
You do NOT decide what to change.
You do NOT re-analyze the task.
Apply the provided plan exactly
`;

if (taskInfo.task_type === "fix" && taskInfo.fault) {
  executionContext += `
--- TARGETED FIX CONTEXT ---
Problem type: ${taskInfo.fault.type}
Affected files: ${taskInfo.fault.files.join(", ")}
Location: ${taskInfo.fault.location}
Summary: ${taskInfo.fault.summary}

Focus ONLY on the affected area.
Do NOT modify unrelated code.
`;
}

if (taskInfo.task_type === "build" || taskInfo.task_type === "feature") {
  executionContext += `
--- BUILD CONTEXT ---
1. Create COMPLETE implementation, not template
2. Add polish and finishing touches
3. Include all features planned above
4. Make it production-ready
5. You may create or modify multiple files.
6. Follow system design best practices.
`;
}
}

const fullPrompt = `
${systemInstruction}

${internalGuidance}

--- CONVERSATION CONTEXT (LAST 2 TURNS) ---
${historyText}

--- CURRENT USER MESSAGE ---
${message}

--- CURRENT PROJECT FILES ---
${filesContext}
`;

/*console.log("==================== FULL PROMPT SENT ====================");
    console.log(fullPrompt);
    console.log("====================================================================");*/

sendStageUpdate(
  clientId,
  "Applying changes..",
  finalModel,
  "is applying changes.."
);

// Ø¨Ø¹Ø¯Ù‡Ø§ ÙŠØ¨Ø¯Ø£ sendResponseUnified + stream

    

    
try {
  await executeWithFallback({
  station: (taskInfo && taskInfo.needs_reasoning) ? "C" : "A",
  prompt: fullPrompt,
  clientId,
  onChunk: (text) => {
    broadcast({ type: "assistant_message", text }, clientId);
  },
  onComplete: (full) => {
    broadcast({
      type: "session_info",
      modelName: combinedModelName || modelConfig.displayName,
      convId
    }, clientId);
    broadcast({ type: "stream_complete" }, clientId);
  }
});
} catch (error) {
  console.error("âŒ All attempts failed including retries:", error.message);
  
  // Ø­ØªÙ‰ Ø¨Ø¹Ø¯ ÙƒÙ„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª ÙØ´Ù„ØªØŒ Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙŠÙ„ Ø¨Ø³ÙŠØ·
  try {
    const simpleModel = getSafeKey('gemini');
    if (simpleModel) {
      console.log("ğŸ†˜ Using emergency simple model...");
      
      const genAI = new GoogleGenerativeAI(simpleModel.token);
      const emergencyModel = genAI.getGenerativeModel({
        model: "gemini-3-flash-preview",
        generationConfig: { 
          maxOutputTokens: 50000,
          temperature: 0.7
        }
      });
      
      const result = await emergencyModel.generateContent(fullPrompt);
      const text = result.response.text();
      
      if (text) {
        broadcast({ type: "assistant_message", text }, clientId);
        broadcast({ 
          type: 'session_info', 
          modelName: "Gemini 3 Flash (Emergency)",
          convId: convId
        }, clientId);
      }
    }
  } catch (finalError) {
    console.error("âŒ Even emergency model failed:", finalError.message);
    // Ù„Ø§ ØªØ±Ø³Ù„ Ø£ÙŠ Ø´ÙŠØ¡ - ØªÙˆÙ‚Ù ØµØ§Ù…ØªØ§Ù‹
  }
}


// Ø¨Ø¹Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ù€ stream

    console.log(`âœ… [SUCCESS] Response completed for ConvID: ${convId}`);
    
      
      console.log(`âœ… ===== REQUEST COMPLETED =====`);
console.log(`ğŸ“Š Final Gemini usage for ${activeKeyInfo.id}: RPM=${usageStats[activeKeyInfo.id].gemini.rpm}, TPM=${usageStats[activeKeyInfo.id].gemini.tpm}`);



    broadcast({ type: "assistant_message", text: "\n[STREAM COMPLETE]" }, clientId);
    
    
    console.log(`\nğŸ” Checking for auto-summary...`);
    const conversation = conversationMemory[convId];
    const isFirstAIResponse = conversation && conversation.messageCount === 0;

console.log(`   Is first AI response: ${isFirstAIResponse}`);
console.log(`   History length: ${conversation?.history?.length || 0}`);
if (isFirstAIResponse) {
    console.log(`ğŸš€ Starting Gemma summarization process...`);
}
     // --- Ø§Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ---
    
    if (conversation && conversation.history && conversation.history.length === 1) {
        // Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø±Ø¯ Ø§Ù„Ø£ÙˆÙ„ ÙÙŠ Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø©
        // Ù†Ø¬Ù…Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„Ø±Ø¯
        const fullAIResponse = conversation.history.join('');
        console.log(`ğŸ“¤ Sending to Gemma summarizer...`);
        console.log(`   AI Response preview: ${fullAIResponse.substring(0, 100)}...`);
        // Ù†Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ø«Ù… Ù†Ø±Ø³Ù„ Ù„Ù„ØªÙ„Ø®ÙŠØµ (ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†)
        setTimeout(async () => {
            try {
                const summary = await summarizeConversationWithGemma(convId, message, fullAIResponse);
                
                if (summary) {
                  console.log(`ğŸ“¨ Broadcasting summary to clients...`);
                    // Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªÙ„Ø®ÙŠØµ Ù„Ù„Ø¹Ù…ÙŠÙ„ Ù„ØªØ­Ø¯ÙŠØ« Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
                    broadcast({ 
                        type: "conversation_summary", 
                        convId: convId,
                        summary: summary
                    });
                    console.log(`âœ… Summary broadcast complete`);
                }
            } catch (error) {
                console.error("Auto-summary process failed:", error);
            }
        }, 1000); // Ø§Ù†ØªØ¸Ø§Ø± 1 Ø«Ø§Ù†ÙŠØ© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø±Ø¯
    }
    res.json({ status: "ok" });
if (conversationMemory[convId].history.length > 20) { // Ø²Ø¯Ù† Ø§Ù„Ø­Ø¯ Ù‚Ù„ÙŠÙ„Ø§Ù‹
        // Ù†Ù‚ÙˆÙ… Ø¨Ø§Ù„ØªÙ„Ø®ÙŠØµ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© Ø¯ÙˆÙ† Ø§Ù†ØªØ¸Ø§Ø±
        

    }

  
  // ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø¯ÙŠØ«
console.log(`ğŸ” Post-request check for ${activeKeyInfo.id}:`);
console.log(`   Gemini RPM: ${usageStats[activeKeyInfo.id].gemini.rpm}`);
console.log(`   Gemini TPM: ${usageStats[activeKeyInfo.id].gemini.tpm}`);
console.log(`   Gemma RPM: ${usageStats[activeKeyInfo.id].gemma.rpm}`);
console.log(`   Gemma TPM: ${usageStats[activeKeyInfo.id].gemma.tpm}`);
// Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø¨Ø¹Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø±Ø¯
if (conversationMemory[convId]) {
    conversationMemory[convId].messageCount = (conversationMemory[convId].messageCount || 0) + 1;
}

  

  } catch (err) {
    console.error(`âŒ ===== REQUEST FAILED =====`);
    console.error(`ğŸ”§ Error details:`, err.message);
    console.error(`ğŸ”§ Stack:`, err.stack?.substring(0, 300));
    console.error("âŒ Generation Error:", err);
    
    
    
    // ØªØµØ­ÙŠØ­ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
    if (activeKeyInfo) {
        usageStats[activeKeyInfo.id].gemini.rpm = Math.max(0, (usageStats[activeKeyInfo.id].gemini.rpm || 0) - 1);
    }
    
    console.error("API Error:", err);
    
    
  }
});

app.get('/api/events', (req, res) => {
  res.set({
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache',
    Connection: 'keep-alive'
  });
  res.flushHeaders();
const clientId = req.query.clientId || Date.now().toString();
  const id = Date.now();
  
    // Ù†Ø­ÙØ¸ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù…Ø¹ Ø§Ù„Ù€ clientId Ø§Ù„Ø®Ø§Øµ Ø¨Ù‡
  const newClient = { id: clientId, clientId: clientId, res };
  clients.push(newClient);
  console.log(`ğŸ”Œ Client connected: ${clientId}`);
  const keepAlive = setInterval(() => {
    res.write(': keep-alive\n\n');
  }, 15000);

  req.on('close', () => {
    clearInterval(keepAlive);
    clients = clients.filter(c => c.id !== clientId);
    console.log(`ğŸ”Œ Client disconnected: ${clientId}`);
  });
});

// ============= Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªÙˆÙ„ÙŠØ¯ Ø´Ø±Ø­ Ø§Ø­ØªÙŠØ§Ø·ÙŠ =============
function generateFallbackExplanation(taskInfo, message) {
  const intent = taskInfo?.intent || "build";
  
  if (intent === "build") {
    return `â€¢ problem: You want to create a new ${message.includes('Ù„Ø¹Ø¨Ø©') ? 'game' : 'project'}
â€¢ cause: You're building something interactive and fun
â€¢ solution: I'll create a complete, polished implementation
â€¢ result: A fully functional product ready to use
â€¢ features: Core functionality, clean UI, smooth animations`;
  }
  
  if (intent === "fix") {
    return `â€¢ problem: There's an issue with your code
â€¢ cause: A bug or unexpected behavior
â€¢ solution: I'll fix the problem and optimize the code
â€¢ result: Your application will work correctly`;
  }
  
  return `â€¢ problem: Processing your request
â€¢ cause: Direct execution
â€¢ solution: Implementing changes
â€¢ result: Completed successfully`;
}

function extractTextFromBrokenJSON(str) {
  // Ø§Ø³ØªØ®Ø±Ø§Ø¬ internal_analysis
  const analysisMatch = str.match(/"internal_analysis"\s*:\s*"([^"]+)"/);
  const analysis = analysisMatch ? analysisMatch[1] : "Failed to parse reasoning response";
  
  // Ø§Ø³ØªØ®Ø±Ø§Ø¬ user_explanation  
  const explanationMatch = str.match(/"user_explanation"\s*:\s*"([^"]+)"/);
  const explanation = explanationMatch ? explanationMatch[1].replace(/\\n/g, '\n') : generateFallbackExplanation(taskInfo, message);
  
  return {
    analysis,
    explanation
  };
}

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => console.log(`Server running on port ${PORT}`));

