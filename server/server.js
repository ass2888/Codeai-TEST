import express from 'express';
import cors from 'cors';
import bodyParser from 'body-parser';
import path from 'path';
import { fileURLToPath } from 'url';
import { dirname } from 'path';
import { GoogleGenerativeAI } from '@google/generative-ai';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

// --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø­Ø¯ÙˆØ¯ (Ù…Ø«Ø§Ù„ Ù„Ù€ Gemini Flash) ---
const LIMITS = {
    GEMINI: {
        RPM: 3,
        TPM: 230000,
        RPD: 17,
    },
    GEMMA: {
        RPM: 27,      // Gemma Ù„Ù‡ Ø­Ø¯ÙˆØ¯ Ø£Ø¹Ù„Ù‰
        TPM: 12000,
        RPD: 12000,
    }
};

// Ø£Ø¶Ù ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø¹Ø¯ ØªØ¹Ø±ÙŠÙ LIMITS
// Ø¨Ø¹Ø¯ ØªØ¹Ø±ÙŠÙ LIMITS Ø£Ø¶Ù:
const MODEL_CONFIGS = {
    'gemini-3-flash': {
        provider: 'google',
        modelName: 'gemini-3-flash-preview',
        displayName: 'Gemini 3 Flash',
        maxTokens: 100000,
        temperature: 0.7,
        supportsStreaming: true,
        features: ['fast', 'latest']
    },
    'gemini-2.5-pro': {
        provider: 'google',
        modelName: 'gemini-2.5-pro-preview-03-25',
        displayName: 'Gemini 2.5 Pro',
        maxTokens: 1000000,
        temperature: 0.7,
        supportsStreaming: false,
        features: ['long-context', 'reasoning', 'advanced']
    },
    'gemini-2.5': {
        provider: 'google',
        modelName: 'gemini-2.5-flash-preview-03-25',
        displayName: 'Gemini 2.5',
        maxTokens: 100000,
        temperature: 0.7,
        supportsStreaming: true,
        features: ['fast', 'efficient', 'balanced']
    },
    'deepseek-coder': {
        provider: 'deepseek',
        modelName: 'deepseek-coder',
        displayName: 'DeepSeek Coder',
        maxTokens: 16000,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://api.deepseek.com/v1/chat/completions',
        features: ['coding', 'open-source']
    },
    'deepseek-chat': {
        provider: 'deepseek',
        modelName: 'deepseek-chat',
        displayName: 'DeepSeek Chat',
        maxTokens: 16000,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://api.deepseek.com/v1/chat/completions',
        features: ['general', 'open-source']
    }
};

const D1 = process.env.D1; // D = deepseek
const G3 = process.env.GEMINI_API_KEY; 
const G2 = process.env.GEMINI_KEY;
const G1 = process.env.G1; // G = Gemini

// ÙƒØ§Ø¦Ù† Ù„ØªØªØ¨Ø¹ Ø§Ù„Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ù„ÙƒÙ„ Ù…ÙØªØ§Ø­
// ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ù…Ù„Ø© Ù„Ù€ usageStats
let usageStats = {};

// ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙØ§ØªÙŠØ­
// ØºÙŠÙ‘Ø± Ø§Ù„Ø³Ø·Ø± Ù„ÙŠØµØ¨Ø­:
['G1', 'G2', 'G3', 'D1'].forEach(keyId => {
    usageStats[keyId] = {
        gemini: { 
            rpm: 0, 
            tpm: 0, 
            rpd: 0, 
            lastMinute: Date.now(), 
            lastDay: Date.now() 
        },
        gemma: { 
            rpm: 0, 
            tpm: 0, 
            rpd: 0, 
            lastMinute: Date.now(), 
            lastDay: Date.now() 
        },
        deepseek: { // â¬… Ø£Ø¶Ù Ù‡Ø°Ø§
            rpm: 0, 
            tpm: 0, 
            rpd: 0, 
            lastMinute: Date.now(), 
            lastDay: Date.now() 
        }
    };
});

console.log("âœ… Initialized usage stats for all keys");

/**
 * Ø¯Ø§Ù„Ø© Ù„ØªØµÙÙŠØ± Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ù†Ø¯ Ù…Ø±ÙˆØ± Ø¯Ù‚ÙŠÙ‚Ø© Ø£Ùˆ ÙŠÙˆÙ…
 */
function refreshStats(keyId, modelType) {
    const now = Date.now();
    const stats = usageStats[keyId][modelType];
    
    // ØªØµÙÙŠØ± Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
    if (now - stats.lastMinute > 60000) {
        stats.rpm = 0;
        stats.tpm = 0;
        stats.lastMinute = now;
    }
    // ØªØµÙÙŠØ± Ø§Ù„ÙŠÙˆÙ…
    if (now - stats.lastDay > 86400000) {
        stats.rpd = 0;
        stats.lastDay = now;
    }
}

/**
 * Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Ø¢Ù…Ù† Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ÙŠÙ†
 */
/**
 * Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Ø¢Ù…Ù† Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ÙŠÙ† (Ù…Ø¹Ø¯Ù„ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªØ¯Ø§Ø®Ù„)
 */
function getSafeKey(modelType = 'gemini') {
    const keys = ['G1', 'G2', 'G3'];
    const limits = LIMITS[modelType.toUpperCase()];
    
    console.log(`ğŸ” Looking for ${modelType} key. Available keys: ${keys}`);
    
    for (let keyId of keys) {
        const keyToken = process.env[keyId];
        if (!keyToken) {
            console.log(`   ${keyId}: No token available`);
            continue;
        }

        refreshStats(keyId, modelType);
        const stats = usageStats[keyId][modelType];

        // ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù‚ÙŠÙ… Ù„ÙŠØ³Øª NaN
        const currentRpm = isNaN(stats.rpm) ? 0 : stats.rpm;
        const currentTpm = isNaN(stats.tpm) ? 0 : stats.tpm;
        const currentRpd = isNaN(stats.rpd) ? 0 : stats.rpd;

        const isRpmSafe = currentRpm < (limits.RPM - 1);
        const isTpmSafe = currentTpm < (limits.TPM * 0.9);
        const isRpdSafe = currentRpd < limits.RPD;
        
        console.log(`   ${keyId}: RPM=${currentRpm}/${limits.RPM}, TPM=${currentTpm}/${limits.TPM}, RPD=${currentRpd}/${limits.RPD}`);
        
        if (isRpmSafe && isTpmSafe && isRpdSafe) {
            console.log(`âœ… Selected ${modelType} Key: ${keyId}`);
            return { 
                id: keyId, 
                token: keyToken,
                modelType: modelType
            };
        } else {
            console.log(`   ${keyId}: Limits exceeded`);
        }
    }
    
    console.log(`âŒ No available keys for ${modelType}`);
    return null;
}

function getSafeKeyForModel(requestedModel) {
    const modelConfig = MODEL_CONFIGS[requestedModel];
    if (!modelConfig) {
        console.log(`âŒ Model config not found: ${requestedModel}`);
        return getSafeKey();
    }
    
    const limits = modelConfig.provider === 'deepseek' ? 
        { RPM: 60, TPM: 1000000, RPD: 1000 } : 
        { RPM: 3, TPM: 230000, RPD: 17 };
    
    let keys = modelConfig.provider === 'deepseek' ? ['D1'] : ['G1', 'G2', 'G3'];
    const modelType = modelConfig.provider === 'deepseek' ? 'deepseek' : 'gemini';
    
    for (let keyId of keys) {
        const keyToken = process.env[keyId];
        if (!keyToken) continue;
        
        refreshStats(keyId, modelType);
        const stats = usageStats[keyId][modelType];
        
        const isRpmSafe = stats.rpm < (limits.RPM - 1);
        const isTpmSafe = stats.tpm < (limits.TPM * 0.9);
        const isRpdSafe = stats.rpd < limits.RPD;
        
        if (isRpmSafe && isTpmSafe && isRpdSafe) {
            return { 
                id: keyId, 
                token: keyToken,
                provider: modelConfig.provider,
                modelConfig: modelConfig
            };
        }
    }
    
    return null;
}

async function sendResponseUnified({
  model,
  prompt,
  supportsStreaming,
  provider,
  keyInfo,
  onChunk,
  onComplete
}) {
  let fullResponse = "";
if (provider === 'google') {
  if (supportsStreaming) {
    // ğŸ”¹ Streaming models (Flash, etc.)
    const result = await model.generateContentStream(prompt);

    for await (const chunk of result.stream) {
      const text = chunk.text();
      if (text) {
        fullResponse += text;
        onChunk(text);
      }
    }
  } else {
    // ğŸ”¹ Non-streaming models (Pro, future models)
    const result = await model.generateContent(prompt);
    const text =
      result.response?.text?.() ||
      result.response?.candidates?.[0]?.content?.parts?.[0]?.text ||
      "";

    if (text) {
      fullResponse = text;
      onChunk(text); // ğŸ‘ˆ Chunk ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·
    }
  }
  } else if (provider === 'deepseek') {
    const messages = [
      { role: "system", content: "You are a helpful AI coding assistant." },
      { role: "user", content: prompt }
    ];
    
    const response = await callDeepSeekAPI(keyInfo, messages);
    
    if (supportsStreaming) {
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim() !== '');
        
        for (const line of lines) {
          if (line.startsWith('data: ') && line.slice(6) !== '[DONE]') {
            try {
              const content = JSON.parse(line.slice(6)).choices[0]?.delta?.content || '';
              if (content) {
                fullResponse += content;
                onChunk(content);
              }
            } catch (e) {}
          }
        }
      }
    } else {
      const data = await response.json();
      const content = data.choices[0]?.message?.content || '';
      if (content) {
        fullResponse = content;
        onChunk(content);
      }
    }
  }

  onComplete(fullResponse);
}


/**
 * ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¹Ø¯ Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©
 */
/**
 * ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¹Ø¯ Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø© (Ù…ØµØ­Ø­)
 */
function updateUsage(keyId, modelType, tokens) {
    if (!usageStats[keyId] || !usageStats[keyId][modelType]) {
        console.error(`âŒ Invalid stats for ${keyId}.${modelType}`);
        return;
    }
    
    const stats = usageStats[keyId][modelType];
    
    // ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† tokens Ø±Ù‚Ù… ØµØ§Ù„Ø­
    const safeTokens = isNaN(parseInt(tokens)) ? 100 : parseInt(tokens);
    
    // Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù‚ÙŠÙ… Ø¥Ø°Ø§ ÙƒØ§Ù†Øª NaN
    if (isNaN(stats.rpm)) stats.rpm = 0;
    if (isNaN(stats.tpm)) stats.tpm = 0;
    if (isNaN(stats.rpd)) stats.rpd = 0;
    
    // Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØµØ­ÙŠØ­Ø©
    stats.rpm += 1;
    stats.rpd += 1;
    stats.tpm += safeTokens;
    
    console.log(`ğŸ“Š Updated ${modelType.toUpperCase()} usage for ${keyId}: RPM=${stats.rpm}, TPM=${stats.tpm}, Tokens=${safeTokens}`);
}

async function callDeepSeekAPI(keyInfo, messages) {
    const response = await fetch(keyInfo.modelConfig.apiUrl, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${keyInfo.token}`
        },
        body: JSON.stringify({
            model: keyInfo.modelConfig.modelName,
            messages: messages,
            max_tokens: keyInfo.modelConfig.maxTokens,
            temperature: keyInfo.modelConfig.temperature,
            stream: keyInfo.modelConfig.supportsStreaming
        })
    });
    
    if (!response.ok) throw new Error(`DeepSeek API error: ${response.status}`);
    return response;
}

/**
 * ØªÙ‚Ø¯ÙŠØ± Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø² Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©
 */
function estimateTokens(text) {
    if (!text || typeof text !== 'string') return 100; // Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¢Ù…Ù†Ø©
    
    // ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ: 1 ØªÙˆÙƒÙ† Ù„ÙƒÙ„ 4 Ø­Ø±ÙˆÙ (ØªÙ‚Ø±ÙŠØ¨ Gemini)
    const tokenCount = Math.ceil(text.length / 4);
    
    // ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù‚ÙŠÙ…Ø© Ø±Ù‚Ù… ØµØ­ÙŠØ­ Ù…ÙˆØ¬Ø¨
    return Math.max(100, tokenCount); // Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ 100 ØªÙˆÙƒÙ† Ù„Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø©
}


async function summarizeConversationWithGemma(convId, userMessage, aiResponse) {
  console.log(`\nğŸ¯ ===== GEMMA SUMMARIZATION STARTED =====`);
  
  // Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Ù„Ù€ Gemma
  const gemmaKeyInfo = getSafeKey('gemma');

  if (!gemmaKeyInfo) {
    console.warn("âš ï¸ No available keys for Gemma summarization");
    console.log(`ğŸ”„ Using fallback summary`);
    const fallback = generateFallbackSummary(userMessage);
    console.log(`ğŸ“ Fallback summary: "${fallback}"`);
    console.log(`âŒ ===== GEMMA SUMMARIZATION SKIPPED =====\n`);
    return fallback;
  }

  console.log(`ğŸ”‘ Using Gemma Key: ${gemmaKeyInfo.id}`);
  console.log(`ğŸ“Š Current Gemma usage: RPM=${usageStats[gemmaKeyInfo.id]?.gemma?.rpm || 0}, TPM=${usageStats[gemmaKeyInfo.id]?.gemma?.tpm || 0}`);

  try {
    const summaryPrompt = `You are an AI conversation summarizer. Your ONLY task is to generate a short, descriptive title for a coding/development conversation.

STRICT RULES:
1. Read ONLY the user's first message and the AI's first response
2. Generate a concise, descriptive title (MAX 40 characters)
3. Use the same language as the conversation (English or Arabic)
4. DO NOT add quotes, punctuation, or emojis
5. DO NOT use "..." truncation - make it a complete phrase
6. DO NOT mention "conversation about" or "discussion of"
7. Make it natural like app conversation titles
8. Focus on the main task/request

CONVERSATION:
User: ${userMessage}
AI: ${aiResponse.substring(0, 200)}

TITLE:`;

    console.log(`ğŸ“‹ Summary prompt length: ${summaryPrompt.length} chars`);
    console.log(`ğŸ“‹ Prompt preview: ${summaryPrompt.substring(0, 150)}...`);

    const response = await fetch(
      'https://generativelanguage.googleapis.com/v1beta/models/gemma-3-1b-it:generateContent',
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-goog-api-key': gemmaKeyInfo.token
        },
        body: JSON.stringify({
          contents: [{
            parts: [{ text: summaryPrompt }]
          }],
          generationConfig: {
            maxOutputTokens: 50,
            temperature: 0.3,
            topP: 0.9,
            topK: 40
          }
        })
      }
    );

    if (!response.ok) {
      const errorText = await response.text();
      console.error(`âŒ Gemma API error: ${response.status} - ${errorText.substring(0, 200)}`);
      throw new Error(`Gemma API error: ${response.status}`);
    }

    const data = await response.json();
    const summary = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
    
    console.log(`ğŸ“ Raw summary from Gemma: "${summary}"`);
    
    // ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ„Ø®ÙŠØµ
    const cleanedSummary = cleanSummary(summary, userMessage);
    
    // ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
    const tokens = estimateTokens(summaryPrompt + cleanedSummary);
    updateUsage(gemmaKeyInfo.id, 'gemma', tokens);
    
    console.log(`âœ… Cleaned summary: "${cleanedSummary}"`);
    return cleanedSummary;
    
  } catch (error) {
    console.error("âŒ Gemma summarization failed:", error);
    // Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¯ÙŠÙ„ Ù…Ø­Ø³Ù†
    
  }
}

function cleanSummary(summary, userMessage) {
  console.log(`ğŸ§¹ Cleaning summary: "${summary}"`);
  
  if (!summary || summary.trim().length === 0) {
    console.log(`ğŸ§¼ Empty summary, using fallback`);
    
  }
  
  // Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ†ØµÙŠØµ ÙˆØ§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©
  let cleaned = summary
    .trim()
    .replace(/^["'`]|["'`]$/g, '')  // Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ†ØµÙŠØµ
    .replace(/^[\[\]\(\)]|[\[\]\(\)]$/g, '') // Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù‚ÙˆØ§Ø³
    .replace(/\.\.\.$/g, '') // Ø¥Ø²Ø§Ù„Ø© "..." Ù…Ù† Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
    .replace(/\s+/g, ' ')    // Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
    .replace(/^\d+\.\s*/, '') // Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
    .trim();
  
  console.log(`ğŸ§¹ After initial clean: "${cleaned}" (${cleaned.length} chars)`);
  
  // Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø·ÙˆÙ„ ÙˆØªØ­Ø³ÙŠÙ†Ù‡
  if (cleaned.length > 40) {
    // Ù…Ø­Ø§ÙˆÙ„Ø© Ø°ÙƒÙŠØ© Ù„Ù„ØªÙ‚ØµÙŠØ± Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… "..."
    cleaned = smartTruncate(cleaned, 40);
    console.log(`âœ‚ï¸ Smart truncated: "${cleaned}" (${cleaned.length} chars)`);
  }
  
  // Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‚ØµÙŠØ±Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ Ø£Ùˆ ÙØ§Ø±ØºØ§Ù‹
  if (cleaned.length < 3) {
    
  }
  
  // ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø£ÙˆÙ„ ÙƒØ¨ÙŠØ± (Ù„Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)
  if (/^[a-z]/.test(cleaned)) {
    cleaned = cleaned.charAt(0).toUpperCase() + cleaned.slice(1);
  }
  
  console.log(`âœ… Final cleaned: "${cleaned}"`);
  return cleaned;
}

/**
 * ØªÙ‚ØµÙŠØ± Ø°ÙƒÙŠ Ù„Ù„Ù†Øµ Ø¯ÙˆÙ† Ù‚Ø·Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
 */
function smartTruncate(text, maxLength) {
  if (text.length <= maxLength) return text;
  
  // Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³Ø§ÙØ© Ù„Ù„Ù‚Ø·Ø¹ Ø¹Ù†Ø¯Ù‡Ø§
  let truncated = text.substring(0, maxLength);
  const lastSpace = truncated.lastIndexOf(' ');
  
  if (lastSpace > maxLength * 0.7) { // Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ù…Ø³Ø§ÙØ© Ù…Ù†Ø§Ø³Ø¨Ø©
    truncated = truncated.substring(0, lastSpace);
  }
  
  // Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ ÙÙˆØ§ØµÙ„ Ø²Ø§Ø¦Ø¯Ø© ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
  truncated = truncated.replace(/[,\-\:;\.\s]+$/, '');
  
  return truncated;
}

/**
 * Ø¯Ø§Ù„Ø© Ø¨Ø¯ÙŠÙ„Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ ØªÙ„Ø®ÙŠØµ ÙÙŠ Ø­Ø§Ù„Ø© ÙØ´Ù„ API
 */
function generateFallbackSummary(userMessage) {
    // Ù„Ø®Øµ Ø£ÙˆÙ„ Ø±Ø³Ø§Ù„Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    let summary = userMessage
        .replace(/[<>]/g, '') // Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª HTML
        .replace(/\n/g, ' ')  // Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø£Ø³Ø·Ø± Ø¨Ù…Ø³Ø§ÙØ§Øª
        .trim();
    
    // Ù‚Ø·Ø¹ Ù„Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
    if (summary.length > 40) {
        summary = summary.substring(0, 37) + '...';
    }
    
    // Ø¥Ø°Ø§ ÙƒØ§Ù† ÙØ§Ø±ØºØ§Ù‹ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ù†ÙˆØ§Ù† Ø§ÙØªØ±Ø§Ø¶ÙŠ
    if (!summary || summary.length < 3) {
        return "New Conversation";
    }
    
    return summary;
}



const app = express();
app.use(cors());
app.use(bodyParser.json({ limit: '10mb' })); // Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø­Ø¯ Ù„Ø§Ø³ØªÙŠØ¹Ø§Ø¨ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©

app.use(express.static(path.join(__dirname, '..', 'client')));

let clients = [];
let conversationMemory = {};

// ==========================================
// 1. ØªØ­Ø¯ÙŠØ« Ø¯Ø§Ù„Ø© broadcast Ù„ØªÙ‚Ø¨Ù„ targetClientId
// ==========================================
function broadcast(data, targetClientId = null) {
  const msg = `data: ${JSON.stringify(data)}\n\n`;
  clients.forEach(c => {
    // Ù†Ø±Ø³Ù„ ÙÙ‚Ø· Ù„Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø°ÙŠ ÙŠØ·Ø§Ø¨Ù‚ Ø§Ù„Ù€ clientId
    // Ø£Ùˆ Ù†Ø±Ø³Ù„ Ù„Ù„Ø¬Ù…ÙŠØ¹ Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ù‡Ø¯Ù (Ù„Ù„ØªÙˆØ§ÙÙ‚ØŒ Ù„ÙƒÙ† ÙŠÙØ¶Ù„ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø§Ù„ØªØ­Ø¯ÙŠØ¯)
    if (!targetClientId || c.clientId === targetClientId) {
      try { c.res.write(msg); }
      catch(e) { console.error("âŒ Broadcast error:", e); }
    }
  });
}

app.post('/api/chat', async (req, res) => {
  // 1. Ù†Ø³ØªÙ‚Ø¨Ù„ Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ÙƒÙˆØ¯ ÙˆØ§Ø­Ø¯
const { message, files, convId, history, settings, clientId } = req.body;
const requestedModel = settings?.selectedModel || 'gemini-3-flash';
  const modelConfig = MODEL_CONFIGS[requestedModel] || MODEL_CONFIGS['gemini-3-flash'];
let finalKeyInfo = getSafeKeyForModel(requestedModel);
  let usedModelName = modelConfig.displayName; 
  let provider = modelConfig.provider;
    
    console.log(`ğŸ¯ Requested model: ${modelConfig.displayName} (Provider: ${modelConfig.provider})`);
console.log(`ğŸ”‘ Using Key: ${finalKeyInfo} for ${provider}`);
    
const optimizedHistory = history.map((msg, index) => {
    if (index >= history.length - 2) {
        return { ...msg, files: [] }; // Ø¥ÙØ±Ø§Øº Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ø¢Ø®Ø± Ø±Ø³Ø§Ù„ØªÙŠÙ†
    }
    
    return msg;
});
console.log("optimizedHistory:", optimizedHistory)



if (!conversationMemory[convId]) {
    conversationMemory[convId] = {
        summary: "",
        history: [], // Ù‡Ø°Ø§ Ø³ÙŠØ®Ø²Ù† Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„ÙƒÙ„ Ø±Ø¯ (Ù„ÙŠØ³ chunks)
        messageCount: 0 // ğŸ‘ˆ Ø¥Ø¶Ø§ÙØ© Ø¹Ø¯Ø§Ø¯ Ù„Ù„Ø±Ø³Ø§Ø¦Ù„
    };
}

const activeKeyInfo = getSafeKey();
    
      // Ù…Ù†Ø·Ù‚ Ø§Ù„Ù€ Fallback (Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø®ØªØ§Ø± Ù†Ø¹ÙˆØ¯ Ù„Ù€ Gemini Flash)
  if (!finalKeyInfo) {
      console.log(`âš ï¸ Primary model keys exhausted. Switching to Fallback...`);
      finalKeyInfo = getSafeKey('gemini'); 
      
      if (!finalKeyInfo) {
          broadcast({ type: 'assistant_message', text: 'âš ï¸ Ø§Ù„Ø³ÙŠØ±ÙØ± Ù…Ø´ØºÙˆÙ„ Ø¬Ø¯Ø§Ù‹.' }, clientId);
          return res.json({ status: 'limit-reached' });
      }
       // ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§Ø³Ù… ÙŠØ¯ÙˆÙŠØ§Ù‹ Ù„Ù„Ù€ Fallback
      usedModelName = "Gemini 3 Flash (Auto)"; 
      provider = 'google';
      
      // Ø¥Ø¹Ø¯Ø§Ø¯ config ÙˆÙ‡Ù…ÙŠ Ù„Ù„Ù€ fallback
      finalKeyInfo.modelConfig = {
          modelName: 'gemini-2.0-flash-exp', // Ø£Ùˆ Ø§Ù„Ù…ØªØ§Ø­ Ù„Ø¯ÙŠÙƒ
          maxTokens: 100000,
          temperature: 0.7,
          supportsStreaming: true
      };
  } else {
      // Ø¥Ø°Ø§ Ù†Ø¬Ø­Ù†Ø§ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø±Ø³Ù…ÙŠ
      usedModelName = finalKeyInfo.modelConfig.displayName;
  }


console.log(`ğŸ¯ ===== GEMINI REQUEST STARTED =====`);
console.log(`ğŸ“Œ Conversation ID: ${convId}`);
console.log(`ğŸ”‘ Using Gemini Key: ${activeKeyInfo.id}`);
console.log(`ğŸ“Š Current Gemini usage: RPM=${usageStats[activeKeyInfo.id].gemini.rpm}, TPM=${usageStats[activeKeyInfo.id].gemini.tpm}`);
console.log(`ğŸ“ User message: ${message.substring(0, 100)}...`);

  

  try {
    let model;
       if (provider === 'google') {
       // Ø¥Ø¹Ø¯Ø§Ø¯ Gemini
       const genAI = new GoogleGenerativeAI(finalKeyInfo.token);
       model = genAI.getGenerativeModel({ 
           model: finalKeyInfo.modelConfig.modelName,
           generationConfig: { 
               maxOutputTokens: finalKeyInfo.modelConfig.maxTokens,
               temperature: finalKeyInfo.modelConfig.temperature
           }
       });
    }

    // Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© ÙØ§Ø±ØºØ© Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ù€ Stream Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙÙ‚Ø·
    broadcast({ type: 'assistant_message', text: ' ' }, clientId);
    
    
    console.log(`ğŸ¤– Selected Gemini model: ${modelConfig.modelName}`);
const estimatedRequestTokens = estimateTokens(message + JSON.stringify(files || ""));

    // ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª (Ø¨Ø´ÙƒÙ„ Ù…Ø¤Ù‚Øª Ù‚Ø¨Ù„ Ø§Ù„Ø·Ù„Ø¨)
        usageStats[activeKeyInfo.id].gemini.rpm += 1;
usageStats[activeKeyInfo.id].gemini.rpd += 1;
usageStats[activeKeyInfo.id].gemini.tpm += estimatedRequestTokens;

    

    // 1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¨ØµØ±ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø«ÙŠÙ… (Dark/Light)
    let visualStyleInstruction = "";
    if (settings && settings.theme === 'light') {
        visualStyleInstruction = `
--- VISUAL STYLE (LIGHT THEME) ---
If the user does not specify a particular design or theme, ALWAYS apply the following default style:
1. Colors:
   - Background: #FFFFFF (Pure White)
   - Secondary/Surface: #E0E0E0
   - Text: #080808 (Deep Black)
   - Accent: #CCCCCC
   - Borders: rgba(0,0,0,0.1)
2. Components:
   - Use distinct shadows (box-shadow: 0 2px 8px rgba(0,0,0,0.05)) for cards.
   - Buttons: Black text on White background or Light Grey.
   - Modals, Cards, and Menus: border-radius: 16px;
   - Buttons: border-radius: 30px; background-color: #000000; color: #080808; (Change colors only if multiple buttons exist to show hierarchy).
3. Typography:
   - For English text, use the 'Archives' font family.
`;
    } else {
        // Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¸Ù„Ù… (Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø§Ù„Ù‚Ø¯ÙŠÙ…)
        visualStyleInstruction = `
--- DEFAULT VISUAL STYLE (DARK THEME) ---
If the user does not specify a particular design or theme, ALWAYS apply the following default style:
1. Colors:
   - Background: #080808 (Deep Black)
   - Secondary/Surface: #2A2A2A
   - Text: #FFFFFF (Pure White)
   - Accent: #333333
2. Typography:
   - For English text, use the 'Archives' font family.
3. Components:
   - Modals, Cards, and Menus: border-radius: 16px;
   - Buttons: border-radius: 30px; background-color: #FFFFFF; color: #000000; (Change colors only if multiple buttons exist to show hierarchy).
`;
    }

    // 2. ØªØ­Ø¯ÙŠØ¯ Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ (Detailed vs Simple)
    let personaInstruction = "";
    if (settings && settings.convStyle === 'Simple') {
        personaInstruction = `
- COMMUNICATION STYLE: SIMPLE & INTERACTIVE -
You are chatting with a non-technical user or someone who wants quick results.
1. DO NOT explain the code in detail.
2. DO NOT list changed files unless asked.
3. Just say enthusiastically: "I've updated the design for you!", "Game is ready!", etc.
4. Be very interactive, ask "Do you want to change the colors?", "Shall we add sound?".
`;
    } else {
        // Detailed (Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ)
        personaInstruction = `
- COMMUNICATION STYLE: DETAILED & EXPERT -
You are chatting with a developer.
1. Briefly explain the technical changes.
2. Be interactive but professional.
`;
    }

    // 3. Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙØ¶Ù„Ø©
    const prefLang = settings && settings.prefLanguage ? settings.prefLanguage : 'HTML';

    // 2. ØªØ­Ø¶ÙŠØ± Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ø¥Ø±Ø³Ø§Ù„Ù‡ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
    let filesContext = "";
    if (files && Array.isArray(files)) {
        filesContext = files.map(f => 
            `--- FILE START: ${f.name} ---\n${f.content}\n--- FILE END: ${f.name} ---`
        ).join("\n\n");
    }
// 3. ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    const systemInstruction = `You are an expert, friendly web developer.

--- INFO ---

- YOUR GOAL -
Help the user by editing existing files or CREATING new files based on their request.
- ABOUT -
1. Identity & Platform:
You are Codeai (in arabic (ÙƒÙˆØ¯Ø§ÙŠ)), an integrated AI chat assistant and code editor. You operate within the Codeai PWA, designed to provide a seamless coding and assistance experience.
2.â€‹Capabilities & Constraints:
You support code generation and live previews for the following languages only: HTML, CSS, JavaScript, Java, Python, PHP, and C++. Ensure all technical solutions and previews align with these supported environments.
3. If the user asks for anything that doesn't relate to coding, answer them normally; you aren't for coding only.

--- USER SETTINGS ---
- Preferred Language: ${prefLang} (Default to this if starting a new project).
- Theme: ${settings?.theme || 'dark'}

${visualStyleInstruction}
4. ALWAYS include the following block at the very beginning of every CSS file or <style> tag:
* {
    -webkit-tap-highlight-color: transparent;
}
5. NEVER use alert(), Make your own modal instead.
6. Try always to add simple animations for buttons, modals, cards, and almost everything that makes the app/game better

${personaInstruction}



--- RULES ---
1. **Language:** Reply in the language the user speaks.
2. **Multi-File Capability:** You can edit multiple files in one response.
3. To ADD new functions/classes (without repeating code): 
   Use <ADD_TO target="filename.ext" position="end">content</ADD_TO> (position can be "start" or "end").
4. For SMALL changes: Use <REPLACE file="filename.ext">
   <<<<<<< SEARCH
   one or two lines ONLY to find
   =======
   new lines
   >>>>>>> REPLACE
   </REPLACE>
5. **New Files:** If the user asks for a new file, output a file block with that name.
6. You can provide multiple <FILE> or <ADD_TO> or <REPLACE> blocks in a single response if the task requires changing multiple files (e.g., updating HTML, CSS, and JS together)."
7. â€‹Dumping & Coding: Place all diffs and code blocks at the absolute end. Ensure any conversational text or questions for the user precede the code markers <>, as anything following them is hidden.
--- OUTPUT FORMAT (STRICT) ---
To create a file, use this EXACT format at the end of your response:

<FILE name="filename.ext">
... FULL code content here ...
</FILE>
`;

 // 5. Ø¯Ù…Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ® (Context)
    // Ø§Ø¨Ø­Ø« Ø¹Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ ÙÙŠ server.js ÙˆØ¹Ø¯Ù„Ù‡ Ù„ÙŠØµØ¨Ø­ Ù‡ÙƒØ°Ø§:
let historyText = "";
if (history && Array.isArray(history)) {
    historyText = history.map(msg => {
        // ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø­Ù‚Ù„ Ø§Ù„ØµØ­ÙŠØ­ (sender Ø£Ùˆ role)
        const role = msg.role || msg.sender || 'user'; 
        const text = msg.text || msg.content || '';
        return `[${role.toUpperCase()}]: ${text.substring(0, 500)}`;
    }).join("\n");
}




    const fullPrompt = `
${systemInstruction}

--- CONVERSATION CONTEXT (LAST 2 TURNS) ---
${historyText}

--- CURRENT USER MESSAGE ---
${message}

--- CURRENT PROJECT FILES ---
${filesContext}
`;

console.log("==================== FULL PROMPT SENT TO GEMINI ====================");
    console.log(fullPrompt);
    console.log("====================================================================");



    

    
await sendResponseUnified({
    model,
    prompt: fullPrompt,
    supportsStreaming: finalKeyInfo.modelConfig.supportsStreaming,
      provider: provider,
    keyInfo: finalKeyInfo,
    onChunk: (text) => {
         if (usageStats[finalKeyInfo.id] && usageStats[finalKeyInfo.id][provider]) {
              usageStats[finalKeyInfo.id][provider].tpm += estimateTokens(text);
          }
          // âœ… Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙÙ‚Ø·
          broadcast({ type: "assistant_message", text }, clientId);
    },
    onComplete: (fullResponse) => {
        if (conversationMemory[convId]) {
              conversationMemory[convId].history.push(fullResponse);
          }
          
          // âœ…âœ…âœ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø±Ø¯ âœ…âœ…âœ…
          broadcast({ 
              type: 'session_info', 
              modelName: usedModelName, // Ø³ÙŠØ¸Ù‡Ø± Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³Ù… ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
              convId: convId
          }, clientId);

    }
});

// Ø¨Ø¹Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ù€ stream

    console.log(`âœ… [SUCCESS] Response completed for ConvID: ${convId}`);
    
      
      console.log(`âœ… ===== GEMINI REQUEST COMPLETED =====`);
console.log(`ğŸ“Š Final Gemini usage for ${activeKeyInfo.id}: RPM=${usageStats[activeKeyInfo.id].gemini.rpm}, TPM=${usageStats[activeKeyInfo.id].gemini.tpm}`);



    broadcast({ type: "assistant_message", text: "\n[STREAM COMPLETE]" }, clientId);
    
    
    console.log(`\nğŸ” Checking for auto-summary...`);
    const conversation = conversationMemory[convId];
    const isFirstAIResponse = conversation && conversation.messageCount === 0;

console.log(`   Is first AI response: ${isFirstAIResponse}`);
console.log(`   History length: ${conversation?.history?.length || 0}`);
if (isFirstAIResponse) {
    console.log(`ğŸš€ Starting Gemma summarization process...`);
}
     // --- Ø§Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ---
    
    if (conversation && conversation.history && conversation.history.length === 1) {
        // Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø±Ø¯ Ø§Ù„Ø£ÙˆÙ„ ÙÙŠ Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø©
        // Ù†Ø¬Ù…Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„Ø±Ø¯
        const fullAIResponse = conversation.history.join('');
        console.log(`ğŸ“¤ Sending to Gemma summarizer...`);
        console.log(`   AI Response preview: ${fullAIResponse.substring(0, 100)}...`);
        // Ù†Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ø«Ù… Ù†Ø±Ø³Ù„ Ù„Ù„ØªÙ„Ø®ÙŠØµ (ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†)
        setTimeout(async () => {
            try {
                const summary = await summarizeConversationWithGemma(convId, message, fullAIResponse);
                
                if (summary) {
                  console.log(`ğŸ“¨ Broadcasting summary to clients...`);
                    // Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªÙ„Ø®ÙŠØµ Ù„Ù„Ø¹Ù…ÙŠÙ„ Ù„ØªØ­Ø¯ÙŠØ« Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
                    broadcast({ 
                        type: "conversation_summary", 
                        convId: convId,
                        summary: summary
                    });
                    console.log(`âœ… Summary broadcast complete`);
                }
            } catch (error) {
                console.error("Auto-summary process failed:", error);
            }
        }, 1000); // Ø§Ù†ØªØ¸Ø§Ø± 1 Ø«Ø§Ù†ÙŠØ© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø±Ø¯
    }
    res.json({ status: "ok" });
if (conversationMemory[convId].history.length > 20) { // Ø²Ø¯Ù† Ø§Ù„Ø­Ø¯ Ù‚Ù„ÙŠÙ„Ø§Ù‹
        // Ù†Ù‚ÙˆÙ… Ø¨Ø§Ù„ØªÙ„Ø®ÙŠØµ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© Ø¯ÙˆÙ† Ø§Ù†ØªØ¸Ø§Ø±
        

    }
  } catch (err) {
    console.error(`âŒ ===== GEMINI REQUEST FAILED =====`);
    console.error(`ğŸ”§ Error details:`, err.message);
    console.error(`ğŸ”§ Stack:`, err.stack?.substring(0, 300));
    console.error("âŒ Generation Error:", err);
    broadcast({ type:'assistant_message', text: `Error: ${err.message}` }, clientId);
    res.json({ status:'error' });
    
    // ØªØµØ­ÙŠØ­ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
    if (activeKeyInfo) {
        usageStats[activeKeyInfo.id].gemini.rpm = Math.max(0, (usageStats[activeKeyInfo.id].gemini.rpm || 0) - 1);
    }
    
    console.error("API Error:", err);
    
    
  }
  
  // ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø¯ÙŠØ«
console.log(`ğŸ” Post-request check for ${activeKeyInfo.id}:`);
console.log(`   Gemini RPM: ${usageStats[activeKeyInfo.id].gemini.rpm}`);
console.log(`   Gemini TPM: ${usageStats[activeKeyInfo.id].gemini.tpm}`);
console.log(`   Gemma RPM: ${usageStats[activeKeyInfo.id].gemma.rpm}`);
console.log(`   Gemma TPM: ${usageStats[activeKeyInfo.id].gemma.tpm}`);
// Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø¨Ø¹Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø±Ø¯
if (conversationMemory[convId]) {
    conversationMemory[convId].messageCount = (conversationMemory[convId].messageCount || 0) + 1;
}
});

app.get('/api/events', (req, res) => {
  res.set({
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache',
    Connection: 'keep-alive'
  });
  res.flushHeaders();
const clientId = req.query.clientId || Date.now().toString();
  const id = Date.now();
  
    // Ù†Ø­ÙØ¸ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù…Ø¹ Ø§Ù„Ù€ clientId Ø§Ù„Ø®Ø§Øµ Ø¨Ù‡
  const newClient = { id: clientId, clientId: clientId, res };
  clients.push(newClient);
  console.log(`ğŸ”Œ Client connected: ${clientId}`);
  const keepAlive = setInterval(() => {
    res.write(': keep-alive\n\n');
  }, 15000);

  req.on('close', () => {
    clearInterval(keepAlive);
    clients = clients.filter(c => c.id !== clientId);
    console.log(`ğŸ”Œ Client disconnected: ${clientId}`);
  });
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => console.log(`Server running on port ${PORT}`));

