import express from 'express';
import cors from 'cors';
import bodyParser from 'body-parser';
import path from 'path';
import { fileURLToPath } from 'url';
import { dirname } from 'path';
import { GoogleGenerativeAI } from '@google/generative-ai';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);
const DEFAULT_REASONING_MODEL = "trinity-large";
const OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions";
const providerHealth = {
  openrouter: { blockedUntil: 0 },
  google: { blockedUntil: 0 }
};

const PROVIDER_COOLDOWN = 60_000; // 60 Ø«Ø§Ù†ÙŠØ©


// === Codeai code-R 1.0 | Fallback Map ===
const STATION_FALLBACKS = {
  A: [ // fast / general
    "gemini-3-flash",
    "solar-pro",
    "gpt-oss"
  ],

  B: [ // deep reasoning
    "trinity-large",
    "chimera-r1",
    "hermes-3",
    "gemini-2.5-pro"
  ],

  C: [ // coding
    "qwen-coder",
    "chimera-r1",
    "solar-pro",
    "gemini-3-flash" // emergency
  ]
};


// --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø­Ø¯ÙˆØ¯ (Ù…Ø«Ø§Ù„ Ù„Ù€ Gemini Flash) ---
const LIMITS = {
    GEMINI: {
        RPM: 3,
        TPM: 230000,
        RPD: 17,
    },
    GEMMA: {
        RPM: 27,      // Gemma Ù„Ù‡ Ø­Ø¯ÙˆØ¯ Ø£Ø¹Ù„Ù‰
        TPM: 12000,
        RPD: 12000,
    },
    OPENROUTER: { // Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ©
        RPM: 20,     
        TPM: 40000,
        RPD: 500,
    },
    KIMI: { // ğŸ‘ˆ Ø¥Ø¶Ø§ÙØ© Ø¬Ø¯ÙŠØ¯Ø©
        RPM: 17,       // Ø¹Ø¯Ù„ Ø­Ø³Ø¨ Ø­Ø¯ÙˆØ¯ Ø­Ø³Ø§Ø¨Ùƒ ÙÙŠ Moonshot
        TPM: 470000,
        RPD: 200,
    }
};

// Ø£Ø¶Ù ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø¹Ø¯ ØªØ¹Ø±ÙŠÙ LIMITS
// Ø¨Ø¹Ø¯ ØªØ¹Ø±ÙŠÙ LIMITS Ø£Ø¶Ù:
const MODEL_CONFIGS = {
    'gemini-3-flash': {
        provider: 'google',
        modelName: 'gemini-3-flash-preview',
        displayName: 'Gemini 3 Flash',
        maxTokens: 100000,
        temperature: 0.7,
        supportsStreaming: true,
        features: ['fast', 'latest']
    },
    'gemini-2.5-pro': {
        provider: 'google',
        modelName: 'gemini-2.5-pro',
        displayName: 'Gemini 2.5 Pro',
        maxTokens: 1000000,
        temperature: 0.7,
        supportsStreaming: false,
        features: ['long-context', 'reasoning', 'advanced']
    },
    'gemini-2.5': {
        provider: 'google',
        modelName: 'gemini-2.5-flash',
        displayName: 'Gemini 2.5',
        maxTokens: 100000,
        temperature: 0.7,
        supportsStreaming: true,
        features: ['fast', 'efficient', 'balanced']
    },
    'deepseek-coder': {
        provider: 'deepseek',
        modelName: 'deepseek-coder',
        displayName: 'DeepSeek Coder',
        maxTokens: 16000,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://api.deepseek.com/v1/chat/completions',
        features: ['coding', 'open-source']
    },
    'deepseek-chat': {
        provider: 'deepseek',
        modelName: 'deepseek-chat',
        displayName: 'DeepSeek Chat',
        maxTokens: 16000,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://api.deepseek.com/v1/chat/completions',
        features: ['general', 'open-source']
    },
      'qwen-coder': {
        provider: 'openrouter',
        modelName: 'qwen/qwen3-coder:free', // ØªØµØ­ÙŠØ­ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø´Ø§Ø¦Ø¹ØŒ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… qwen/qwen3-coder:free Ø¥Ø°Ø§ ØªÙˆÙØ±
        displayName: 'Qwen 3 Coder 480B',
        maxTokens: 32000,
        temperature: 0.6,
        supportsStreaming: true,
        apiUrl: 'https://openrouter.ai/api/v1/chat/completions',
        features: ['coding']
    },
    'chimera-r1': {
        provider: 'openrouter',
        modelName: 'tngtech/deepseek-r1t2-chimera:free',
        displayName: 'DeepSeek (Chimera R1T2)',
        maxTokens: 32000,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://openrouter.ai/api/v1/chat/completions',
        features: ['reasoning',]
    },
    'hermes-3': {
        provider: 'openrouter',
        modelName: 'nousresearch/hermes-3-llama-3.1-405b:free',
        displayName: 'Hermes 3 405B',
        maxTokens: 4096,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://openrouter.ai/api/v1/chat/completions',
        features: ['large', 'free']
    },
    'gpt-oss': {
        provider: 'openrouter',
        modelName: 'openai/gpt-oss-20b:free',
        displayName: 'GPT-OSS 20B',
        maxTokens: 4096,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://openrouter.ai/api/v1/chat/completions',
        features: ['free']
    },
    'solar-pro': {
        provider: 'openrouter',
        modelName: 'upstage/solar-pro-3:free',
        displayName: 'Solar Pro 3',
        maxTokens: 4096,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://openrouter.ai/api/v1/chat/completions',
        features: ['efficient', 'free']
    },
    'trinity-large': {
        provider: 'openrouter',
        modelName: 'arcee-ai/trinity-large-preview:free',
        displayName: 'Trinity Large 400B',
        maxTokens: 4096,
        temperature: 0.7,
        supportsStreaming: true,
        apiUrl: 'https://openrouter.ai/api/v1/chat/completions',
        features: ['advanced', 'free']
    },
    'kimi-k2': {
        provider: "kimi",
        modelName: "moonshot-v1-8k",
        maxTokens: 8000,
        temperature: 0.3,
        supportsStreaming: false,
        displayName: "Kimi K2"
}
};

const D1 = process.env.D1; // D = deepseek
const G3 = process.env.G3; 
const G2 = process.env.G2;
const G1 = process.env.G1; // G = Gemini
const O1 = process.env.O1; // O = Openrouter
const O2 = process.env.O2;
const K1 = process.env.K1;

// ÙƒØ§Ø¦Ù† Ù„ØªØªØ¨Ø¹ Ø§Ù„Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ù„ÙƒÙ„ Ù…ÙØªØ§Ø­
// ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ù…Ù„Ø© Ù„Ù€ usageStats
let usageStats = {};

// ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙØ§ØªÙŠØ­
// ØºÙŠÙ‘Ø± Ø§Ù„Ø³Ø·Ø± Ù„ÙŠØµØ¨Ø­:
['G1', 'G2', 'G3', 'D1', 'O1', 'O2', 'K1'].forEach(keyId => {
    usageStats[keyId] = {
        gemini: { rpm: 0, tpm: 0, rpd: 0, lastMinute: Date.now(), lastDay: Date.now() },
        gemma: { rpm: 0, tpm: 0, rpd: 0, lastMinute: Date.now(), lastDay: Date.now() },
        deepseek: { rpm: 0, tpm: 0, rpd: 0, lastMinute: Date.now(), lastDay: Date.now() },
        openrouter: { rpm: 0, tpm: 0, rpd: 0, lastMinute: Date.now(), lastDay: Date.now() }, // â¬… Ø¬Ø¯ÙŠØ¯
        kimi: { rpm: 0, tpm: 0, rpd: 0, lastMinute: Date.now(), lastDay: Date.now() }
    };
});

console.log("âœ… Initialized usage stats for all keys");

/**
 * Ø¯Ø§Ù„Ø© Ù„ØªØµÙÙŠØ± Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ù†Ø¯ Ù…Ø±ÙˆØ± Ø¯Ù‚ÙŠÙ‚Ø© Ø£Ùˆ ÙŠÙˆÙ…
 */
function refreshStats(keyId, modelType) {
    const now = Date.now();
    const stats = usageStats[keyId][modelType];
    
    // ØªØµÙÙŠØ± Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
    if (now - stats.lastMinute > 60000) {
        stats.rpm = 0;
        stats.tpm = 0;
        stats.lastMinute = now;
    }
    // ØªØµÙÙŠØ± Ø§Ù„ÙŠÙˆÙ…
    if (now - stats.lastDay > 86400000) {
        stats.rpd = 0;
        stats.lastDay = now;
    }
}


function isProviderAvailable(provider) {
  return Date.now() > (providerHealth[provider]?.blockedUntil || 0);
}

function markProviderRateLimited(provider) {
  providerHealth[provider] = {
    blockedUntil: Date.now() + PROVIDER_COOLDOWN
  };
}

function selectModelForStation(stationKey) {
  const candidates = STATION_FALLBACKS[stationKey];
  if (!candidates) return null;

  for (const modelId of candidates) {
    const config = MODEL_CONFIGS[modelId];
    if (!config) continue;

    if (isProviderAvailable(config.provider)) {
      return modelId;
    }
  }

  return null; // ÙƒÙ„ Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ† Ù…Ø­Ø¬ÙˆØ¨ÙŠÙ†
}

function getNextFallbackModel(stationKey, currentModel) {
  const list = STATION_FALLBACKS[stationKey];
  if (!list) return null;

  const idx = list.indexOf(currentModel);
  return list[idx + 1] || null;
}

function getNextAvailableModel(startModel) {
  let found = false;

  for (const station of FALLBACK_STATIONS) {
    for (const modelId of station) {
      if (modelId === startModel) {
        found = true;
      }
      if (!found) continue;

      const config = MODEL_CONFIGS[modelId];
      if (!config) continue;

      if (!isProviderAvailable(config.provider)) {
        continue;
      }

      const key = getSafeKeyForModel(modelId);
      if (key) {
        return modelId;
      }
    }
  }

  return null;
}

/**
 * Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Ø¢Ù…Ù† Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ÙŠÙ†
 */
/**
 * Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Ø¢Ù…Ù† Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ÙŠÙ† (Ù…Ø¹Ø¯Ù„ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªØ¯Ø§Ø®Ù„)
 */
function getSafeKey(modelType = 'gemini') {
    const keys = ['G1', 'G2', 'G3'];
    const limits = LIMITS[modelType.toUpperCase()];
    
    console.log(`ğŸ” Looking for ${modelType} key. Available keys: ${keys}`);
    
    for (let keyId of keys) {
        const keyToken = process.env[keyId];
        if (!keyToken) {
            console.log(`   ${keyId}: No token available`);
            continue;
        }

        refreshStats(keyId, modelType);
        const stats = usageStats[keyId][modelType];

        // ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù‚ÙŠÙ… Ù„ÙŠØ³Øª NaN
        const currentRpm = isNaN(stats.rpm) ? 0 : stats.rpm;
        const currentTpm = isNaN(stats.tpm) ? 0 : stats.tpm;
        const currentRpd = isNaN(stats.rpd) ? 0 : stats.rpd;

        const isRpmSafe = currentRpm < (limits.RPM - 1);
        const isTpmSafe = currentTpm < (limits.TPM * 0.9);
        const isRpdSafe = currentRpd < limits.RPD;
        
        console.log(`   ${keyId}: RPM=${currentRpm}/${limits.RPM}, TPM=${currentTpm}/${limits.TPM}, RPD=${currentRpd}/${limits.RPD}`);
        
        if (isRpmSafe && isTpmSafe && isRpdSafe) {
            console.log(`âœ… Selected ${modelType} Key: ${keyId}`);
            return { 
                id: keyId, 
                token: keyToken,
                modelType: modelType
            };
        } else {
            console.log(`   ${keyId}: Limits exceeded`);
        }
    }
    
    console.log(`âŒ No available keys for ${modelType}`);
    
}

function getSafeKeyForModel(requestedModel) {
    const modelConfig = MODEL_CONFIGS[requestedModel];
    if (!modelConfig) {
        console.log(`âŒ Model config not found: ${requestedModel}`);
        return getSafeKey();
    }
    
    // ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ ÙˆØ§Ù„Ù…ÙØ§ØªÙŠØ­ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø²ÙˆØ¯
    let limits, keys, modelType;
    
    if (modelConfig.provider === 'deepseek') {
        limits = { RPM: 60, TPM: 1000000, RPD: 1000 };
        keys = ['D1'];
        modelType = 'deepseek';
    } else if (modelConfig.provider === 'openrouter') { // â¬… Ø­Ø§Ù„Ø© OpenRouter
        limits = LIMITS.OPENROUTER;
        keys = ['O1', 'O2'];
        modelType = 'openrouter';
    } else if (modelConfig.provider === 'kimi') { // ğŸ‘ˆ Ø¥Ø¶Ø§ÙØ© Ø´Ø±Ø· Ø¬Ø¯ÙŠØ¯
        limits = LIMITS.KIMI;
        keys = ['K1'];
        modelType = 'kimi'; // ØªØ£ÙƒØ¯ Ø£Ù†Ùƒ Ø£Ø¶ÙØª Ù‡Ø°Ø§ Ø§Ù„Ù†ÙˆØ¹ ÙÙŠ usageStats ÙÙŠ Ø§Ù„Ø®Ø·ÙˆØ© 1
    } else { // Google
        limits = { RPM: 3, TPM: 230000, RPD: 17 };
        keys = ['G1', 'G2', 'G3'];
        modelType = 'gemini';
    }
    
    for (let keyId of keys) {
        const keyToken = process.env[keyId];
        if (!keyToken) continue;
        
        refreshStats(keyId, modelType);
        const stats = usageStats[keyId][modelType];
        
        const isRpmSafe = stats.rpm < (limits.RPM - 1);
        const isTpmSafe = stats.tpm < (limits.TPM * 0.9);
        const isRpdSafe = stats.rpd < limits.RPD;
        
        if (isRpmSafe && isTpmSafe && isRpdSafe) {
            return { 
                id: keyId, 
                token: keyToken,
                provider: modelConfig.provider,
                modelConfig: modelConfig
            };
        }
    }

}

async function sendResponseUnified({
  model,
  prompt,
  supportsStreaming,
  provider,
  keyInfo,
  onChunk,
  onComplete,
  maxRetries = 2 // Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù…Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
}) {
  let fullResponse = "";
  let retries = 0;
  
  while (retries <= maxRetries) {
    try {
      if (provider === 'google') {
        if (supportsStreaming) {
          const result = await model.generateContentStream(prompt);
          for await (const chunk of result.stream) {
            const text = chunk.text();
            if (text) {
              fullResponse += text;
              onChunk(text);
            }
          }
        } else {
          const result = await model.generateContent(prompt);
          const text = result.response?.text?.() || "";
          if (text) {
            fullResponse = text;
            onChunk(text);
          }
        }
      } else if (provider === 'deepseek' || provider === 'openrouter') {
        const messages = [
          { role: "system", content: "You are a helpful AI coding assistant." },
          { role: "user", content: prompt }
        ];
        
        const apiFunction = provider === 'openrouter' ? callOpenRouterAPI : callDeepSeekAPI;
        const response = await apiFunction(keyInfo, messages);
        
        if (supportsStreaming) {
          const reader = response.body.getReader();
          const decoder = new TextDecoder();
          
          while (true) {
            const { done, value } = await reader.read();
            if (done) break;
            
            const chunk = decoder.decode(value);
            const lines = chunk.split('\n').filter(line => line.trim() !== '');
            
            for (const line of lines) {
              if (line.startsWith('data: ') && line.slice(6) !== '[DONE]') {
                try {
                  const content = JSON.parse(line.slice(6)).choices[0]?.delta?.content || '';
                  if (content) {
                    fullResponse += content;
                    onChunk(content);
                  }
                } catch (e) {}
              }
            }
          }
        } else {
          const data = await response.json();
          const content = data.choices[0]?.message?.content || '';
          if (content) {
            fullResponse = content;
            onChunk(content);
          }
        }
      }
      else if (provider === 'kimi') {
          const kimiResponse = await callKimiAPI({
            token: keyInfo.token,
            modelConfig: keyInfo.modelConfig,
            prompt: prompt
        });
        
        if (kimiResponse) {
            fullResponse = kimiResponse;
            onChunk(kimiResponse); // Kimi Ø­Ø§Ù„ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„Ù€ streamØŒ Ù†Ø±Ø³Ù„ Ø§Ù„Ù†Øµ ÙƒØ§Ù…Ù„Ø§Ù‹
            
  };
}
      // Ø¥Ø°Ø§ ÙˆØµÙ„Ù†Ø§ Ù‡Ù†Ø§ØŒ ÙÙ‚Ø¯ Ù†Ø¬Ø­Ù†Ø§
      onComplete(fullResponse);
      return;
      
    } catch (error) {
      retries++;
      console.error(`âŒ Attempt ${retries}/${maxRetries + 1} failed:`, error.message);
      
      if (retries <= maxRetries) {
        console.log(`ğŸ”„ Retrying... (${retries}/${maxRetries})`);
        // Ø§Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù‚Ø¨Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
        await new Promise(resolve => setTimeout(resolve, 500));
      } else {
        // Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª ÙØ´Ù„Øª
        console.error(`âŒ All ${maxRetries + 1} attempts failed`);
        throw error;
      }
    }
  }
}

async function callKimiAPI({ token, modelConfig, prompt }) {
  const res = await fetch("https://api.moonshot.ai/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      "Authorization": `Bearer ${token}`
    },
    body: JSON.stringify({
      model: modelConfig.modelName,
      messages: [{ role: "user", content: prompt }],
      temperature: modelConfig.temperature,
      max_tokens: modelConfig.maxTokens
    })
  });

  if (!res.ok) {
    const err = await res.text();
    throw new Error(`Kimi API error: ${err}`);
  }

  const data = await res.json();
  return data.choices[0].message.content;
}

async function callOpenRouterAPI(keyInfo, messages) {
    const response = await fetch(OPENROUTER_API_URL, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${keyInfo.token}`,
            'HTTP-Referer': 'https://codeai.app',
            'X-Title': 'Codeai'
        },
        body: JSON.stringify({
            model: keyInfo.modelConfig.modelName,
            messages,
            max_tokens: keyInfo.modelConfig.maxTokens,
            temperature: keyInfo.modelConfig.temperature,
            stream: keyInfo.modelConfig.supportsStreaming
        })
    });

    if (!response.ok) {
        const errText = await response.text();
        markProviderRateLimited('openrouter');
        throw new Error(`OpenRouter API error: ${response.status} - ${errText}`);
    }

    return response; // âœ… Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…
}



function getFallbackModel(originalModelKey) {
    const fallbackMap = {
        // Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù€ keys Ù…Ù† MODEL_CONFIGS
        'hermes-3': 'trinity-large',
        'trinity-large': 'chimera-r1',
        'chimera-r1': 'solar-pro',
        'solar-pro': 'gpt-oss',
        'gpt-oss': 'gemini-3-flash'
    };
    
    return fallbackMap[originalModelKey] || 'gemini-3-flash';
}

/**
 * ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¹Ø¯ Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©
 */
/**
 * ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¹Ø¯ Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø© (Ù…ØµØ­Ø­)
 */
function updateUsage(keyId, modelType, tokens) {
    if (!usageStats[keyId] || !usageStats[keyId][modelType]) {
        console.error(`âŒ Invalid stats for ${keyId}.${modelType}`);
        return;
    }
    
    const stats = usageStats[keyId][modelType];
    
    // ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† tokens Ø±Ù‚Ù… ØµØ§Ù„Ø­
    const safeTokens = isNaN(parseInt(tokens)) ? 100 : parseInt(tokens);
    
    // Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù‚ÙŠÙ… Ø¥Ø°Ø§ ÙƒØ§Ù†Øª NaN
    if (isNaN(stats.rpm)) stats.rpm = 0;
    if (isNaN(stats.tpm)) stats.tpm = 0;
    if (isNaN(stats.rpd)) stats.rpd = 0;
    
    // Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØµØ­ÙŠØ­Ø©
    stats.rpm += 1;
    stats.rpd += 1;
    stats.tpm += safeTokens;
    
    console.log(`ğŸ“Š Updated ${modelType.toUpperCase()} usage for ${keyId}: RPM=${stats.rpm}, TPM=${stats.tpm}, Tokens=${safeTokens}`);
}

async function callDeepSeekAPI(keyInfo, messages) {
    const response = await fetch(keyInfo.modelConfig.apiUrl, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${keyInfo.token}`
        },
        body: JSON.stringify({
            model: keyInfo.modelConfig.modelName,
            messages: messages,
            max_tokens: keyInfo.modelConfig.maxTokens,
            temperature: keyInfo.modelConfig.temperature,
            stream: keyInfo.modelConfig.supportsStreaming
        })
    });
    
    if (!response.ok) throw new Error(`DeepSeek API error: ${response.status}`);
    return response;
}

/**
 * ØªÙ‚Ø¯ÙŠØ± Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø² Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¢Ù…Ù†Ø©
 */
function estimateTokens(text) {
    if (!text || typeof text !== 'string') return 100; // Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¢Ù…Ù†Ø©
    
    // ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ: 1 ØªÙˆÙƒÙ† Ù„ÙƒÙ„ 4 Ø­Ø±ÙˆÙ (ØªÙ‚Ø±ÙŠØ¨ Gemini)
    const tokenCount = Math.ceil(text.length / 4);
    
    // ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù‚ÙŠÙ…Ø© Ø±Ù‚Ù… ØµØ­ÙŠØ­ Ù…ÙˆØ¬Ø¨
    return Math.max(100, tokenCount); // Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ 100 ØªÙˆÙƒÙ† Ù„Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø©
}

async function routeModel(userMessage) {
  const routerPrompt = `
You are a routing AI.

You MUST respond with ONLY valid JSON.
NO explanations.
NO markdown.
NO extra text.

Response format (STRICT):
{"recommended_model":"MODEL_ID"}

Available models:

1) gemini-3-flash
- Best for: fast replies, casual chat, simple questions
- Strengths: very fast, low cost
- Weaknesses: limited deep reasoning

2) gemini-2.5-pro
- Best for: deep reasoning, analysis, complex logic
- Strengths: strong thinking and planning
- Weaknesses: slower than flash

3) gemini-2.5-flash
- Best for: balanced reasoning and speed
- Strengths: good general-purpose model
- Weaknesses: not best for heavy coding

4) qwen-coder
- Best for: coding, debugging, refactoring, file edits
- Strengths: excellent code understanding
- Weaknesses: slower for casual chat

5) chimera-r1
- Best for: research-level reasoning, multi-step analysis
- Strengths: very deep thinking
- Weaknesses: slower, higher cost

6) hermes-3
- Best for: long conversations, explanations, structured output
- Strengths: strong instruction following
- Weaknesses: not code-specialized

7) gpt-oss
- Best for: general knowledge, explanations
- Strengths: stable, predictable
- Weaknesses: weaker than top-tier reasoning

8) solar-pro
- Best for: multilingual chat, balanced tasks
- Strengths: good Arabic + English
- Weaknesses: not best at heavy reasoning

9) trinity-large
- Best for: very complex reasoning and planning
- Strengths: extremely powerful
- Weaknesses: slow, expensive

Rules:
- Choose ONLY from the listed model_id values.
- Prefer specialized models over general ones.
- If unsure, choose gemini-3-flash.

User message:
"${userMessage}"
`;

  const genAI = new GoogleGenerativeAI(getSafeKeyForModel("gemma").token);

  const model = genAI.getGenerativeModel({
    model: "gemma-3-12b-it",
    generationConfig: {
      temperature: 0,
      maxOutputTokens: 50
    }
  });

  const result = await model.generateContent(routerPrompt);
  

  let text = result.response.text().trim();

// Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON ÙÙ‚Ø·
const jsonMatch = text.match(/\{[\s\S]*\}/);

if (!jsonMatch) {
  console.error("âŒ No JSON found in router response:", text);
  return { recommended_model: "gemini-3-flash" };
}

try {
  return JSON.parse(jsonMatch[0]);
} catch (e) {
  console.error("âŒ Router JSON parse failed:", jsonMatch[0]);
  return { recommended_model: "gemini-3-flash" };
}
}

async function summarizeConversationWithGemma(convId, userMessage, aiResponse) {
  console.log(`\nğŸ¯ ===== GEMMA SUMMARIZATION STARTED =====`);
  
  // Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Ù„Ù€ Gemma
  const gemmaKeyInfo = getSafeKey('gemma');

  if (!gemmaKeyInfo) {
    console.warn("âš ï¸ No available keys for Gemma summarization");
    console.log(`ğŸ”„ Using fallback summary`);
    const fallback = generateFallbackSummary(userMessage);
    console.log(`ğŸ“ Fallback summary: "${fallback}"`);
    console.log(`âŒ ===== GEMMA SUMMARIZATION SKIPPED =====\n`);
    return fallback;
  }

  console.log(`ğŸ”‘ Using Gemma Key: ${gemmaKeyInfo.id}`);
  console.log(`ğŸ“Š Current Gemma usage: RPM=${usageStats[gemmaKeyInfo.id]?.gemma?.rpm || 0}, TPM=${usageStats[gemmaKeyInfo.id]?.gemma?.tpm || 0}`);

  try {
    const summaryPrompt = `You are an AI conversation summarizer. Your ONLY task is to generate a short, descriptive title for a coding/development conversation.

STRICT RULES:
1. Read ONLY the user's first message and the AI's first response
2. Generate a concise, descriptive title (MAX 40 characters)
3. Use the same language as the conversation (English or Arabic)
4. DO NOT add quotes, punctuation, or emojis
5. DO NOT use "..." truncation - make it a complete phrase
6. DO NOT mention "conversation about" or "discussion of"
7. Make it natural like app conversation titles
8. Focus on the main task/request

CONVERSATION:
User: ${userMessage}
AI: ${aiResponse.substring(0, 200)}

TITLE:`;

    console.log(`ğŸ“‹ Summary prompt length: ${summaryPrompt.length} chars`);
    console.log(`ğŸ“‹ Prompt preview: ${summaryPrompt.substring(0, 150)}...`);

    const response = await fetch(
      'https://generativelanguage.googleapis.com/v1beta/models/gemma-3-1b-it:generateContent',
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-goog-api-key': gemmaKeyInfo.token
        },
        body: JSON.stringify({
          contents: [{
            parts: [{ text: summaryPrompt }]
          }],
          generationConfig: {
            maxOutputTokens: 50,
            temperature: 0.3,
            topP: 0.9,
            topK: 40
          }
        })
      }
    );

    if (!response.ok) {
      const errorText = await response.text();
      console.error(`âŒ Gemma API error: ${response.status} - ${errorText.substring(0, 200)}`);
      throw new Error(`Gemma API error: ${response.status}`);
    }

    const data = await response.json();
    const summary = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
    
    console.log(`ğŸ“ Raw summary from Gemma: "${summary}"`);
    
    // ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ„Ø®ÙŠØµ
    const cleanedSummary = cleanSummary(summary, userMessage);
    
    // ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
    const tokens = estimateTokens(summaryPrompt + cleanedSummary);
    updateUsage(gemmaKeyInfo.id, 'gemma', tokens);
    
    console.log(`âœ… Cleaned summary: "${cleanedSummary}"`);
    return cleanedSummary;
    
  } catch (error) {
    console.error("âŒ Gemma summarization failed:", error);
    // Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¯ÙŠÙ„ Ù…Ø­Ø³Ù†
    
  }
}

function cleanSummary(summary, userMessage) {
  console.log(`ğŸ§¹ Cleaning summary: "${summary}"`);
  
  if (!summary || summary.trim().length === 0) {
    console.log(`ğŸ§¼ Empty summary, using fallback`);
    
  }
  
  // Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ†ØµÙŠØµ ÙˆØ§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©
  let cleaned = summary
    .trim()
    .replace(/^["'`]|["'`]$/g, '')  // Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ†ØµÙŠØµ
    .replace(/^[\[\]\(\)]|[\[\]\(\)]$/g, '') // Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù‚ÙˆØ§Ø³
    .replace(/\.\.\.$/g, '') // Ø¥Ø²Ø§Ù„Ø© "..." Ù…Ù† Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
    .replace(/\s+/g, ' ')    // Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
    .replace(/^\d+\.\s*/, '') // Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
    .trim();
  
  console.log(`ğŸ§¹ After initial clean: "${cleaned}" (${cleaned.length} chars)`);
  
  // Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø·ÙˆÙ„ ÙˆØªØ­Ø³ÙŠÙ†Ù‡
  if (cleaned.length > 40) {
    // Ù…Ø­Ø§ÙˆÙ„Ø© Ø°ÙƒÙŠØ© Ù„Ù„ØªÙ‚ØµÙŠØ± Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… "..."
    cleaned = smartTruncate(cleaned, 40);
    console.log(`âœ‚ï¸ Smart truncated: "${cleaned}" (${cleaned.length} chars)`);
  }
  
  // Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‚ØµÙŠØ±Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ Ø£Ùˆ ÙØ§Ø±ØºØ§Ù‹
  if (cleaned.length < 3) {
    
  }
  
  // ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø£ÙˆÙ„ ÙƒØ¨ÙŠØ± (Ù„Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)
  if (/^[a-z]/.test(cleaned)) {
    cleaned = cleaned.charAt(0).toUpperCase() + cleaned.slice(1);
  }
  
  console.log(`âœ… Final cleaned: "${cleaned}"`);
  return cleaned;
}

/**
 * ØªÙ‚ØµÙŠØ± Ø°ÙƒÙŠ Ù„Ù„Ù†Øµ Ø¯ÙˆÙ† Ù‚Ø·Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
 */
function smartTruncate(text, maxLength) {
  if (text.length <= maxLength) return text;
  
  // Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³Ø§ÙØ© Ù„Ù„Ù‚Ø·Ø¹ Ø¹Ù†Ø¯Ù‡Ø§
  let truncated = text.substring(0, maxLength);
  const lastSpace = truncated.lastIndexOf(' ');
  
  if (lastSpace > maxLength * 0.7) { // Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ù…Ø³Ø§ÙØ© Ù…Ù†Ø§Ø³Ø¨Ø©
    truncated = truncated.substring(0, lastSpace);
  }
  
  // Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ ÙÙˆØ§ØµÙ„ Ø²Ø§Ø¦Ø¯Ø© ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
  truncated = truncated.replace(/[,\-\:;\.\s]+$/, '');
  
  return truncated;
}

/**
 * Ø¯Ø§Ù„Ø© Ø¨Ø¯ÙŠÙ„Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ ØªÙ„Ø®ÙŠØµ ÙÙŠ Ø­Ø§Ù„Ø© ÙØ´Ù„ API
 */
function generateFallbackSummary(userMessage) {
    // Ù„Ø®Øµ Ø£ÙˆÙ„ Ø±Ø³Ø§Ù„Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    let summary = userMessage
        .replace(/[<>]/g, '') // Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª HTML
        .replace(/\n/g, ' ')  // Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø£Ø³Ø·Ø± Ø¨Ù…Ø³Ø§ÙØ§Øª
        .trim();
    
    // Ù‚Ø·Ø¹ Ù„Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
    if (summary.length > 40) {
        summary = summary.substring(0, 37) + '...';
    }
    
    // Ø¥Ø°Ø§ ÙƒØ§Ù† ÙØ§Ø±ØºØ§Ù‹ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ù†ÙˆØ§Ù† Ø§ÙØªØ±Ø§Ø¶ÙŠ
    if (!summary || summary.length < 3) {
        return "New Conversation";
    }
    
    return summary;
}



const app = express();
app.use(cors());
app.use(bodyParser.json({ limit: '10mb' })); // Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø­Ø¯ Ù„Ø§Ø³ØªÙŠØ¹Ø§Ø¨ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©

app.use(express.static(path.join(__dirname, '..', 'client')));

let clients = [];
let conversationMemory = {};

// ==========================================
// 1. ØªØ­Ø¯ÙŠØ« Ø¯Ø§Ù„Ø© broadcast Ù„ØªÙ‚Ø¨Ù„ targetClientId
// ==========================================
function broadcast(data, targetClientId = null) {
  const msg = `data: ${JSON.stringify(data)}\n\n`;
  clients.forEach(c => {
    // Ù†Ø±Ø³Ù„ ÙÙ‚Ø· Ù„Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø°ÙŠ ÙŠØ·Ø§Ø¨Ù‚ Ø§Ù„Ù€ clientId
    // Ø£Ùˆ Ù†Ø±Ø³Ù„ Ù„Ù„Ø¬Ù…ÙŠØ¹ Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ù‡Ø¯Ù (Ù„Ù„ØªÙˆØ§ÙÙ‚ØŒ Ù„ÙƒÙ† ÙŠÙØ¶Ù„ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø§Ù„ØªØ­Ø¯ÙŠØ¯)
    if (!targetClientId || c.clientId === targetClientId) {
      try { c.res.write(msg); }
      catch(e) { console.error("âŒ Broadcast error:", e); }
    }
  });
}

function getNextFallback(station, currentModel) {
  const list = STATION_MAP[station];
  const index = list.indexOf(currentModel);
  return list[index + 1] || null;
}

function sendStageUpdate(clientId, userText, modelId, actionText) {
  const modelName = MODEL_CONFIGS[modelId]?.displayName || modelId || "AI";

  broadcast({
    type: "assistant_message",
    stage: true,
    text: `${userText}\n${modelName} ${actionText}`
  }, clientId);
}

async function classifyTaskAndThinker(userMessage, files) {
  const prompt = `
You are a task analysis AI.

Your job:
1) Classify the user intent.
2) Decide if deep reasoning is needed.
3) Select the best reasoning model.
4) If the task is FIX, identify where the problem is.

Return ONLY valid JSON in this exact format:
{
  "intent": "build | fix | improve | refactor | explain",
  "needs_reasoning": true | false,
  "reasoning_model": "MODEL_ID or null",
  "fault": {
    "type": "logic | syntax | performance | ui | unknown",
    "files": [],
    "location": "",
    "summary": ""
  }
}

Rules:
- If intent is NOT "fix", set fault = null
- Be concise.
- Do NOT solve the problem.
- Do NOT write code.

Available models: 
gemini-3-flash
gemini-2.5-pro
gemini-2.5-flash
trinity-large
chimera-r1
hermes-3
gpt-oss
solar-pro



User message:
"${userMessage}"
`;

  const genAI = new GoogleGenerativeAI(
    getSafeKeyForModel("gemini-3-flash").token
  );

  const model = genAI.getGenerativeModel({
    model: "gemini-3-flash-preview",
    generationConfig: {
      temperature: 0,
      maxOutputTokens: 150
    }
  });

  const result = await model.generateContent(prompt);
  const text = result.response.text().trim();

  const match = text.match(/\{[\s\S]*\}/);
  if (!match) {
    return {
      intent: "explain",
      needs_reasoning: false,
      reasoning_model: null
    };
  }

  try {
    return JSON.parse(match[0]);
  } catch {
    return {
      intent: "explain",
      needs_reasoning: false,
      reasoning_model: null
    };
  }
}

async function internalReasoning(taskInfo, message, files) {
  const prompt = `
You are a senior software architect.

Your task has TWO outputs:

1) INTERNAL_ANALYSIS
- Deep technical reasoning.
- Root causes.
- Strategy.
- Risks.

2) USER_EXPLANATION
- Simplified explanation for the user.
- No chain-of-thought.

Return ONLY valid JSON:
{
  "internal_analysis": "...",
  "user_explanation": {
    "problem": "...",
    "cause": "...",
    "solution": "...",
    "result": "..."
  }
}

Task intent: ${taskInfo.intent}

User message:
"${message}"
`;

  const modelId = taskInfo.reasoning_model;
  const modelConfig = MODEL_CONFIGS[modelId];

  if (!modelConfig) {
    throw new Error(`Reasoning model not found: ${modelId}`);
  }
console.log("task:", taskInfo)
console.log("modelConfig:", modelConfig)
  const keyInfo = getSafeKeyForModel(modelId);
  if (!keyInfo) {
    throw new Error("No available key for reasoning model");
  }

  let text = "";

  // ğŸ”¹ Google
  if (modelConfig.provider === "google") {
    const genAI = new GoogleGenerativeAI(keyInfo.token);
    const model = genAI.getGenerativeModel({
      model: modelConfig.modelName,
      generationConfig: {
        temperature: 0.2,
        maxOutputTokens: 800
      }
    });

    const result = await model.generateContent(prompt);
    text = result.response.text().trim();
  }

  // ğŸ”¹ OpenRouter
  if (modelConfig.provider === "openrouter") {
    const messages = [
      { role: "system", content: "You are a senior software architect." },
      { role: "user", content: prompt }
    ];

    // 2. ØªÙ…Ø±ÙŠØ± keyInfo ÙƒÙ…ØªØºÙŠØ± Ù…Ø³ØªÙ‚Ù„ ØªÙ…Ø§Ù…Ø§Ù‹
    // Ù†Ù†ØªØ¸Ø± Ø§Ù„Ù†Øµ Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ø£Ù† callOpenRouterAPI ØªØ¹ÙŠØ¯ content
    const result = await callOpenRouterAPI(keyInfo, messages);
  

    text = result.trim();
  }

  if (!text) {
    throw new Error("Reasoning model returned empty response");
  }

  const match = text.match(/\{[\s\S]*\}/);
  if (!match) {
    console.log("text: ", text)
    throw new Error("No JSON found in reasoning response");
  }
  

  try {
    return JSON.parse(match[0]);
  } catch (err) {
    throw new Error("Failed to parse reasoning JSON");
  }
}


function isAutoMode(modelId) {
    return modelId === "auto" 
        || modelId === "codeai-code-r";
}

async function executeWithFallback({
  station,
  prompt,
  clientId,
  onChunk,
  onComplete
}) {
  let currentModel = selectModelForStation(station);

  while (currentModel) {
    const modelConfig = MODEL_CONFIGS[currentModel];
    const keyInfo = getSafeKeyForModel(currentModel);

    if (!keyInfo) {
      currentModel = getNextFallbackModel(station, currentModel);
      continue;
    }

    try {
      await sendResponseUnified({
        model: modelConfig.provider === "google"
          ? new GoogleGenerativeAI(keyInfo.token).getGenerativeModel({
              model: modelConfig.modelName,
              generationConfig: {
                maxOutputTokens: modelConfig.maxTokens,
                temperature: modelConfig.temperature
              }
            })
          : null,
        provider: modelConfig.provider,
        keyInfo,
        prompt,
        supportsStreaming: modelConfig.supportsStreaming,
        onChunk,
        onComplete,
        maxRetries: 0
      });

      return; // âœ… Ù†Ø¬Ø­ Ø§Ù„ØªÙ†ÙÙŠØ°

    } catch (err) {
      console.warn(`âš ï¸ Execution failed on ${currentModel}:`, err.message);

      if (err.message === 'RATE_LIMITED') {
        markProviderRateLimited(modelConfig.provider);
        currentModel = getNextFallbackModel(station, currentModel);
        continue;
      }

      throw err; // Ø£Ø®Ø·Ø§Ø¡ Ø­Ù‚ÙŠÙ‚ÙŠØ©
    }
  }

  throw new Error("ALL_EXECUTION_MODELS_FAILED");
}

app.post('/api/chat', async (req, res) => {
  // 1. Ù†Ø³ØªÙ‚Ø¨Ù„ Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ÙƒÙˆØ¯ ÙˆØ§Ø­Ø¯
const { message, files, convId, history, settings, clientId } = req.body;
const requestedModel = settings?.selectedModel || 'gemini-3-flash';
const isAutoPro = requestedModel === "codeai-code-r";
let finalModel = requestedModel;
let taskInfo = null;
const modelConfig =
  MODEL_CONFIGS[finalModel] || MODEL_CONFIGS['gemini-3-flash'];
// ===== Auto PRO override (CRITICAL) =====



if (requestedModel === "auto") {
  const route = await routeModel(message);
  finalModel = route.recommended_model;
  console.log("Final model selected :", finalModel)
};
if (requestedModel === "codeai-code-r") {
  taskInfo = await classifyTaskAndThinker(message, files);
  sendStageUpdate(
  clientId,
  "Analyzing your request..",
  "gemini-3-flash",
  "is analyzing.."
);
if (isAutoPro) {

  // 1ï¸âƒ£ Ù„Ø§ Ù†Ø³Ù…Ø­ Ø¨Ù€ explain ÙÙŠ PRO
  if (taskInfo.intent === "explain") {
    taskInfo.intent = "build";
  }

  // 2ï¸âƒ£ Auto PRO Ø¯Ø§Ø¦Ù…Ù‹Ø§ reasoning
  taskInfo.needs_reasoning = true;

  // 3ï¸âƒ£ Ø¶Ù…Ø§Ù† ÙˆØ¬ÙˆØ¯ Ù†Ù…ÙˆØ°Ø¬ ØªÙÙƒÙŠØ±
  if (!taskInfo.reasoning_model) {
    taskInfo.reasoning_model = "trinity-large";
  }
}

  console.log("ğŸ§  Auto PRO analysis:", taskInfo);
}
let reasoningData = null;
let reasoningSummary = null;
let userExplanation = null;
let combinedModelName = null;

if (taskInfo?.needs_reasoning) {
  sendStageUpdate(
    clientId,
    "Identifying the problem..",
    taskInfo.reasoning_model,
    "is thinking.."
  );
let reasoningData = null;

// ğŸ”¸ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
if (taskInfo.needs_reasoning) {
  try {
    sendStageUpdate(
      clientId,
      "Identifying the problem..",
      taskInfo.reasoning_model,
      "is thinking.."
    );

    reasoningData = await internalReasoning(taskInfo, message, files);

  } catch (err) {
    console.warn("âš ï¸ Reasoning failed:", err.message);
  }
}

// ğŸ”¸ Fallback ØªÙ„Ù‚Ø§Ø¦ÙŠ
if (!reasoningData && taskInfo.needs_reasoning) {
  console.log("ğŸ” Reasoning fallback â†’ Gemini Flash");

  taskInfo.reasoning_model = "gemini-2.5";

  try {
    sendStageUpdate(
      clientId,
      "Identifying the problem..",
      taskInfo.reasoning_model,
      "is thinking.."
    );

    let currentModel = taskInfo.reasoning_model;

while (currentModel) {
  try {
    taskInfo.reasoning_model = currentModel;
    return await internalReasoning(taskInfo, message, files);
  } catch (err) {
    console.warn(`âš ï¸ Model failed: ${currentModel}`, err.message);

    if (err.message === 'RATE_LIMITED') {
      const nextModel = getNextAvailableModel(currentModel);
      if (!nextModel) break;
      currentModel = nextModel;
      continue;
    }

    throw err;
  }
}
} catch (error) {
  /* handle error */
  }


// ğŸ”¸ Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ØªÙˆÙ‚Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
if (!reasoningData) {
  reasoningData = {
    internal_analysis: "",
    user_explanation: {
      problem: "",
      cause: "",
      solution: "Proceeding directly with execution.",
      result: ""
    }
  };
}
  

const reasoningModelName = MODEL_CONFIGS[taskInfo.reasoning_model]?.displayName || "Reasoner";
 combinedModelName = `${reasoningModelName} + ${modelConfig.displayName}`;

  reasoningData = await internalReasoning(taskInfo, message, files);

  if (reasoningData) {
    reasoningSummary = reasoningData.internal_analysis;
    userExplanation = reasoningData.user_explanation;

    // >>>> Ø¥Ø¶Ø§ÙØ©: Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®ØªØµØ± ÙÙˆØ±Ø§Ù‹ Ù„Ù„Ø¹Ù…ÙŠÙ„ <<<<
    broadcast({
        type: 'thought_process',
        text: reasoningSummary || "No analysis details provided."
    }, clientId);
  }
}


// Ù„Ùˆ Ø§Ù„ÙˆØ¶Ø¹ Auto / PRO â†’ Ù„Ø§ Ù†Ø·Ù„Ø¨ Ù…ÙØªØ§Ø­ Ø§Ù„Ø¢Ù†
let finalKeyInfo = null;

if (!isAutoMode(finalModel)) {
    finalKeyInfo = getSafeKeyForModel(finalModel);
}
let usedModelName = modelConfig.displayName;
let provider = modelConfig.provider;
  
  
if (requestedModel === "auto" || requestedModel === "codeai-code-r") {
    const routeResult = await routeModel(message, files);

    finalModel = routeResult.recommended_model; // Ù…Ø«Ù„ qwen-coder
    console.log("reasoning model:", finalModel)
    const candidateProvider = MODEL_CONFIGS[finalModel].provider;

if (!isProviderAvailable(candidateProvider)) {
  throw new Error(`PROVIDER_BLOCKED:${candidateProvider}`);
}

provider = candidateProvider;

    finalKeyInfo = getSafeKeyForModel(finalModel);
}

    console.log(`ğŸ¯ Requested model: ${modelConfig.displayName} (Provider: ${modelConfig.provider})`);
console.log(`ğŸ”‘ Using Key: ${finalKeyInfo} for ${provider}`);
    
const optimizedHistory = history.map((msg, index) => {
    if (index >= history.length - 2) {
        return { ...msg, files: [] }; // Ø¥ÙØ±Ø§Øº Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ø¢Ø®Ø± Ø±Ø³Ø§Ù„ØªÙŠÙ†
    }
    
    return msg;
});
console.log("optimizedHistory:", optimizedHistory)



if (!conversationMemory[convId]) {
    conversationMemory[convId] = {
        summary: "",
        history: [], // Ù‡Ø°Ø§ Ø³ÙŠØ®Ø²Ù† Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„ÙƒÙ„ Ø±Ø¯ (Ù„ÙŠØ³ chunks)
        messageCount: 0 // ğŸ‘ˆ Ø¥Ø¶Ø§ÙØ© Ø¹Ø¯Ø§Ø¯ Ù„Ù„Ø±Ø³Ø§Ø¦Ù„
    };
}

const activeKeyInfo = getSafeKey();
    
      // Ù…Ù†Ø·Ù‚ Ø§Ù„Ù€ Fallback (Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø®ØªØ§Ø± Ù†Ø¹ÙˆØ¯ Ù„Ù€ Gemini Flash)
  if (!finalKeyInfo) {
      console.log(`âš ï¸ Primary model keys exhausted. Switching to Fallback...`);
      finalKeyInfo = getSafeKey('gemini'); 
      
      if (!finalKeyInfo) {
          broadcast({ type: 'assistant_message', text: 'âš ï¸ Ø§Ù„Ø³ÙŠØ±ÙØ± Ù…Ø´ØºÙˆÙ„ Ø¬Ø¯Ø§Ù‹.' }, clientId);
          return res.json({ status: 'limit-reached' });
      }
       // ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§Ø³Ù… ÙŠØ¯ÙˆÙŠØ§Ù‹ Ù„Ù„Ù€ Fallback
      usedModelName = "Gemini 3 Flash (Auto)"; 
      provider = 'google';
      
      // Ø¥Ø¹Ø¯Ø§Ø¯ config ÙˆÙ‡Ù…ÙŠ Ù„Ù„Ù€ fallback
      finalKeyInfo.modelConfig = {
          modelName: 'gemini-2.5-flash', // Ø£Ùˆ Ø§Ù„Ù…ØªØ§Ø­ Ù„Ø¯ÙŠÙƒ
          maxTokens: 100000,
          temperature: 0.7,
          supportsStreaming: true
      };
  } else {
      // Ø¥Ø°Ø§ Ù†Ø¬Ø­Ù†Ø§ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø±Ø³Ù…ÙŠ
      usedModelName = finalKeyInfo.modelConfig.displayName;
  }


console.log(`ğŸ¯ ===== REQUEST STARTED =====`);
console.log(`ğŸ“Œ Conversation ID: ${convId}`);
console.log(`ğŸ”‘ Using Key: ${activeKeyInfo.id}`);

console.log(`ğŸ“ User message: ${message.substring(0, 100)}...`);

  

  try {
    let model;
       if (provider === 'google') {
       // Ø¥Ø¹Ø¯Ø§Ø¯ Gemini
       const genAI = new GoogleGenerativeAI(finalKeyInfo.token);
       model = genAI.getGenerativeModel({ 
           model: finalKeyInfo.modelConfig.modelName,
           generationConfig: { 
               maxOutputTokens: finalKeyInfo.modelConfig.maxTokens,
               temperature: finalKeyInfo.modelConfig.temperature
           }
       });
    }

    // Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© ÙØ§Ø±ØºØ© Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ù€ Stream Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙÙ‚Ø·
   // broadcast({ type: 'assistant_message', text: ' ' }, clientId);
    
    

const estimatedRequestTokens = estimateTokens(message + JSON.stringify(files || ""));

    // ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª (Ø¨Ø´ÙƒÙ„ Ù…Ø¤Ù‚Øª Ù‚Ø¨Ù„ Ø§Ù„Ø·Ù„Ø¨)
        usageStats[activeKeyInfo.id].gemini.rpm += 1;
usageStats[activeKeyInfo.id].gemini.rpd += 1;
usageStats[activeKeyInfo.id].gemini.tpm += estimatedRequestTokens;

    

    // 1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¨ØµØ±ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø«ÙŠÙ… (Dark/Light)
    let visualStyleInstruction = "";
    if (settings && settings.theme === 'light') {
        visualStyleInstruction = `
- (LIGHT THEME)
If the user does not specify a particular design or theme, ALWAYS apply the following default style:
1. Colors:
   - Background: #FFFFFF (Pure White)
   - Secondary/Surface: #E0E0E0
   - Text: #080808 (Deep Black)
   - Accent: #CCCCCC
   - Borders: rgba(0,0,0,0.1)
2. Components:
   - Use distinct shadows (box-shadow: 0 2px 8px rgba(0,0,0,0.05)) for cards.
   - Buttons: Black text on White background or Light Grey.
   - Modals, Cards, and Menus: border-radius: 16px;
   - Buttons: border-radius: 30px; background-color: #000000; color: #080808; (Change colors only if multiple buttons exist to show hierarchy).
3. Typography:
   - For English text, use the 'Archives' font family.
`;
    } else {
        // Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¸Ù„Ù… (Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø§Ù„Ù‚Ø¯ÙŠÙ…)
        visualStyleInstruction = `
- (DARK THEME)
If the user does not specify a particular design or theme, ALWAYS apply the following default style:
1. Colors:
   - Background: #080808 (Deep Black)
   - Secondary/Surface: #2A2A2A
   - Text: #FFFFFF (Pure White)
   - Accent: #333333
2. Typography:
   - For English text, use the 'Archives' font family.
3. Components:
   - Modals, Cards, and Menus: border-radius: 16px;
   - Buttons: border-radius: 30px; background-color: #FFFFFF; color: #000000; (Change colors only if multiple buttons exist to show hierarchy).
`;
    }

    // 2. ØªØ­Ø¯ÙŠØ¯ Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ (Detailed vs Simple)
    let personaInstruction = "";
    if (settings && settings.convStyle === 'Simple') {
        personaInstruction = `
- COMMUNICATION STYLE: SIMPLE & INTERACTIVE -
You are chatting with a non-technical user or someone who wants quick results.
1. DO NOT explain the code in detail.
2. DO NOT list changed files unless asked.
3. Just say enthusiastically: "I've updated the design for you!", "Game is ready!", etc.
4. Be very interactive, ask "Do you want to change the colors?", "Shall we add sound?".
`;
    } else {
        // Detailed (Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ)
        personaInstruction = `
- COMMUNICATION STYLE: DETAILED & EXPERT -
You are chatting with a developer.
1. Briefly explain the technical changes.
2. Be interactive but professional.
`;
    }

    // 3. Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙØ¶Ù„Ø©
    const prefLang = settings && settings.prefLanguage ? settings.prefLanguage : 'HTML';

    // 2. ØªØ­Ø¶ÙŠØ± Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ø¥Ø±Ø³Ø§Ù„Ù‡ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
    let filesContext = "";
    if (files && Array.isArray(files)) {
        filesContext = files.map(f => 
            `--- FILE START: ${f.name} ---\n${f.content}\n--- FILE END: ${f.name} ---`
        ).join("\n\n");
    }
    let taskModeInstruction = "";

if (taskInfo) {
  switch (taskInfo.intent) {
    case "build":
      taskModeInstruction = `
- TASK MODE: BUILD -
You are creating new features or a new project.
Focus on structure, clarity, and completeness.
`;
      break;

    case "fix":
      taskModeInstruction = `
- TASK MODE: FIX -
You are fixing a bug.
Make minimal, targeted changes.
Do NOT refactor unless necessary.
`;
      break;

    case "improve":
      taskModeInstruction = `
- TASK MODE: IMPROVE -
Enhance existing functionality without breaking behavior.
`;
      break;

    case "refactor":
      taskModeInstruction = `
- TASK MODE: REFACTOR -
Improve code quality and structure without changing functionality.
`;
      break;
  }
}
// 3. ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    const systemInstruction = `You are Codeai Execution Engine.



Your job is to APPLY changes exactly as instructed.
You do NOT decide what to change.
You do NOT re-analyze the task.

--------------------------------------------------
GLOBAL IDENTITY
--------------------------------------------------
- You are Codeai (ÙƒÙˆØ¯Ø§ÙŠ)
- You are not a general chatbot

--------------------------------------------------
USER CONTEXT
--------------------------------------------------
Language: {Arabic | English}
Conversation style: {Simple | Detailed}
Theme: {Dark | Light}
User level: {Developer | Non-technical}

--------------------------------------------------
YOUR ROLE (EXECUTION CONTRACT)
--------------------------------------------------
You MUST:
- Apply the provided plan exactly
- Follow Codeai file output format
- Respect user preferences and theme
- Produce correct, working code

You MUST NOT:
- Change unrelated code
- Re-design the solution
- Re-explain analysis unless allowed

--------------------------------------------------
TASK CONTEXT
--------------------------------------------------
Task type: {build | fix | modify}
Affected scope: {ui | logic | full}

If fixing:
- Only modify the affected files and locations
- Focus strictly on the reported issue

--------------------------------------------------
DESIGN CONTEXT (if UI involved)
--------------------------------------------------
${visualStyleInstruction}
- ALWAYS include the following block at the very beginning of every CSS file or <style> tag:
* {
    -webkit-tap-highlight-color: transparent;
}
- NEVER use alert(), Make your own modal instead.
--------------------------------------------------
RESPONSE STYLE
--------------------------------------------------
If conversation style = Detailed:
- Briefly explain what was done

If conversation style = Simple:
- Minimal confirmation only

--------------------------------------------------
STRICT OUTPUT RULES
--------------------------------------------------
- All code changes MUST be at the end
- Use ONLY:
  <FILE>, <REPLACE>, <ADD_TO>
- No text after code blocks

${taskModeInstruction}
`;

 // 5. Ø¯Ù…Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ® (Context)
    // Ø§Ø¨Ø­Ø« Ø¹Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ ÙÙŠ server.js ÙˆØ¹Ø¯Ù„Ù‡ Ù„ÙŠØµØ¨Ø­ Ù‡ÙƒØ°Ø§:
let historyText = "";
if (history && Array.isArray(history)) {
    historyText = history.map(msg => {
        // ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø­Ù‚Ù„ Ø§Ù„ØµØ­ÙŠØ­ (sender Ø£Ùˆ role)
        const role = msg.role || msg.sender || 'user'; 
        const text = msg.text || msg.content || '';
        return `[${role.toUpperCase()}]: ${text.substring(0, 500)}`;
    }).join("\n");
}




let internalGuidance = "";

if (reasoningSummary) {
  internalGuidance = `
--- INTERNAL GUIDANCE (DO NOT EXPOSE TO USER) ---
${reasoningSummary}
`;
}
let executionContext =''

if (taskInfo) {
executionContext = `
--- EXECUTION TASK ---
You are fixing an existing issue.
`;

if (taskInfo.task_type === "fix" && taskInfo.fault) {
  executionContext += `
--- TARGETED FIX CONTEXT ---
Problem type: ${taskInfo.fault.type}
Affected files: ${taskInfo.fault.files.join(", ")}
Location: ${taskInfo.fault.location}
Summary: ${taskInfo.fault.summary}

Focus ONLY on the affected area.
Do NOT modify unrelated code.
`;
}

if (taskInfo.task_type === "build" || taskInfo.task_type === "feature") {
  executionContext += `
--- BUILD CONTEXT ---
You are building new functionality.
You may create or modify multiple files.
Follow system design best practices.
`;
}
}

const fullPrompt = `
${systemInstruction}

${internalGuidance}

--- CONVERSATION CONTEXT (LAST 2 TURNS) ---
${historyText}

--- CURRENT USER MESSAGE ---
${message}

--- CURRENT PROJECT FILES ---
${filesContext}
`;

/*console.log("==================== FULL PROMPT SENT ====================");
    console.log(fullPrompt);
    console.log("====================================================================");*/

sendStageUpdate(
  clientId,
  "Applying changes..",
  finalModel,
  "is applying changes.."
);

// Ø¨Ø¹Ø¯Ù‡Ø§ ÙŠØ¨Ø¯Ø£ sendResponseUnified + stream

    

    
try {
  await executeWithFallback({
  station: taskInfo?.needs_reasoning ? "C" : "A",
  prompt: fullPrompt,
  clientId,
  onChunk: (text) => {
    broadcast({ type: "assistant_message", text }, clientId);
  },
  onComplete: (full) => {
    broadcast({
      type: "session_info",
      modelName: combinedModelName,
      convId
    }, clientId);
  }
});
} catch (error) {
  console.error("âŒ All attempts failed including retries:", error.message);
  
  // Ø­ØªÙ‰ Ø¨Ø¹Ø¯ ÙƒÙ„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª ÙØ´Ù„ØªØŒ Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙŠÙ„ Ø¨Ø³ÙŠØ·
  try {
    const simpleModel = getSafeKey('gemini');
    if (simpleModel) {
      console.log("ğŸ†˜ Using emergency simple model...");
      
      const genAI = new GoogleGenerativeAI(simpleModel.token);
      const emergencyModel = genAI.getGenerativeModel({
        model: "gemini-3-flash-preview",
        generationConfig: { 
          maxOutputTokens: 50000,
          temperature: 0.7
        }
      });
      
      const result = await emergencyModel.generateContent(fullPrompt);
      const text = result.response.text();
      
      if (text) {
        broadcast({ type: "assistant_message", text }, clientId);
        broadcast({ 
          type: 'session_info', 
          modelName: "Gemini 3 Flash (Emergency)",
          convId: convId
        }, clientId);
      }
    }
  } catch (finalError) {
    console.error("âŒ Even emergency model failed:", finalError.message);
    // Ù„Ø§ ØªØ±Ø³Ù„ Ø£ÙŠ Ø´ÙŠØ¡ - ØªÙˆÙ‚Ù ØµØ§Ù…ØªØ§Ù‹
  }
}


// Ø¨Ø¹Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ù€ stream

    console.log(`âœ… [SUCCESS] Response completed for ConvID: ${convId}`);
    
      
      console.log(`âœ… ===== REQUEST COMPLETED =====`);
console.log(`ğŸ“Š Final Gemini usage for ${activeKeyInfo.id}: RPM=${usageStats[activeKeyInfo.id].gemini.rpm}, TPM=${usageStats[activeKeyInfo.id].gemini.tpm}`);



    broadcast({ type: "assistant_message", text: "\n[STREAM COMPLETE]" }, clientId);
    
    
    console.log(`\nğŸ” Checking for auto-summary...`);
    const conversation = conversationMemory[convId];
    const isFirstAIResponse = conversation && conversation.messageCount === 0;

console.log(`   Is first AI response: ${isFirstAIResponse}`);
console.log(`   History length: ${conversation?.history?.length || 0}`);
if (isFirstAIResponse) {
    console.log(`ğŸš€ Starting Gemma summarization process...`);
}
     // --- Ø§Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ---
    
    if (conversation && conversation.history && conversation.history.length === 1) {
        // Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø±Ø¯ Ø§Ù„Ø£ÙˆÙ„ ÙÙŠ Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø©
        // Ù†Ø¬Ù…Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„Ø±Ø¯
        const fullAIResponse = conversation.history.join('');
        console.log(`ğŸ“¤ Sending to Gemma summarizer...`);
        console.log(`   AI Response preview: ${fullAIResponse.substring(0, 100)}...`);
        // Ù†Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ø«Ù… Ù†Ø±Ø³Ù„ Ù„Ù„ØªÙ„Ø®ÙŠØµ (ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†)
        setTimeout(async () => {
            try {
                const summary = await summarizeConversationWithGemma(convId, message, fullAIResponse);
                
                if (summary) {
                  console.log(`ğŸ“¨ Broadcasting summary to clients...`);
                    // Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªÙ„Ø®ÙŠØµ Ù„Ù„Ø¹Ù…ÙŠÙ„ Ù„ØªØ­Ø¯ÙŠØ« Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
                    broadcast({ 
                        type: "conversation_summary", 
                        convId: convId,
                        summary: summary
                    });
                    console.log(`âœ… Summary broadcast complete`);
                }
            } catch (error) {
                console.error("Auto-summary process failed:", error);
            }
        }, 1000); // Ø§Ù†ØªØ¸Ø§Ø± 1 Ø«Ø§Ù†ÙŠØ© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø±Ø¯
    }
    res.json({ status: "ok" });
if (conversationMemory[convId].history.length > 20) { // Ø²Ø¯Ù† Ø§Ù„Ø­Ø¯ Ù‚Ù„ÙŠÙ„Ø§Ù‹
        // Ù†Ù‚ÙˆÙ… Ø¨Ø§Ù„ØªÙ„Ø®ÙŠØµ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© Ø¯ÙˆÙ† Ø§Ù†ØªØ¸Ø§Ø±
        

    }

  
  // ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø¯ÙŠØ«
console.log(`ğŸ” Post-request check for ${activeKeyInfo.id}:`);
console.log(`   Gemini RPM: ${usageStats[activeKeyInfo.id].gemini.rpm}`);
console.log(`   Gemini TPM: ${usageStats[activeKeyInfo.id].gemini.tpm}`);
console.log(`   Gemma RPM: ${usageStats[activeKeyInfo.id].gemma.rpm}`);
console.log(`   Gemma TPM: ${usageStats[activeKeyInfo.id].gemma.tpm}`);
// Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø¨Ø¹Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø±Ø¯
if (conversationMemory[convId]) {
    conversationMemory[convId].messageCount = (conversationMemory[convId].messageCount || 0) + 1;
}

  

  } catch (err) {
    console.error(`âŒ ===== REQUEST FAILED =====`);
    console.error(`ğŸ”§ Error details:`, err.message);
    console.error(`ğŸ”§ Stack:`, err.stack?.substring(0, 300));
    console.error("âŒ Generation Error:", err);
    
    
    
    // ØªØµØ­ÙŠØ­ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
    if (activeKeyInfo) {
        usageStats[activeKeyInfo.id].gemini.rpm = Math.max(0, (usageStats[activeKeyInfo.id].gemini.rpm || 0) - 1);
    }
    
    console.error("API Error:", err);
    
    
  }
  
});

app.get('/api/events', (req, res) => {
  res.set({
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache',
    Connection: 'keep-alive'
  });
  res.flushHeaders();
const clientId = req.query.clientId || Date.now().toString();
  const id = Date.now();
  
    // Ù†Ø­ÙØ¸ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù…Ø¹ Ø§Ù„Ù€ clientId Ø§Ù„Ø®Ø§Øµ Ø¨Ù‡
  const newClient = { id: clientId, clientId: clientId, res };
  clients.push(newClient);
  console.log(`ğŸ”Œ Client connected: ${clientId}`);
  const keepAlive = setInterval(() => {
    res.write(': keep-alive\n\n');
  }, 15000);

  req.on('close', () => {
    clearInterval(keepAlive);
    clients = clients.filter(c => c.id !== clientId);
    console.log(`ğŸ”Œ Client disconnected: ${clientId}`);
  });
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => console.log(`Server running on port ${PORT}`));

